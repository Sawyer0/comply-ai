---
# Source: llama-mapper/templates/resourcequota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mapper-llama-mapper-quota
  labels:
    helm.sh/chart: llama-mapper-0.1.0
    app.kubernetes.io/name: llama-mapper
    app.kubernetes.io/instance: mapper
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  hard:
    configmaps: "20"
    limits.cpu: "4"
    limits.memory: 8Gi
    persistentvolumeclaims: "0"
    pods: "10"
    requests.cpu: "2"
    requests.memory: 4Gi
    services: "5"
---
# Source: llama-mapper/templates/limitrange.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: mapper-llama-mapper-limits
  labels:
    helm.sh/chart: llama-mapper-0.1.0
    app.kubernetes.io/name: llama-mapper
    app.kubernetes.io/instance: mapper
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  limits:
    - type: Container
      defaultRequest:
        cpu: 250m
        memory: 512Mi
      default:
        cpu: "1"
        memory: 1Gi
---
# Source: llama-mapper/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mapper-llama-mapper
  labels:
    helm.sh/chart: llama-mapper-0.1.0
    app.kubernetes.io/name: llama-mapper
    app.kubernetes.io/instance: mapper
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
---
# Source: llama-mapper/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mapper-llama-mapper
  labels:
    helm.sh/chart: llama-mapper-0.1.0
    app.kubernetes.io/name: llama-mapper
    app.kubernetes.io/instance: mapper
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: llama-mapper
    app.kubernetes.io/instance: mapper
  ports:
    - name: http
      port: 8000
      targetPort: http
      protocol: TCP
---
# Source: llama-mapper/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mapper-llama-mapper
  labels:
    helm.sh/chart: llama-mapper-0.1.0
    app.kubernetes.io/name: llama-mapper
    app.kubernetes.io/instance: mapper
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llama-mapper
      app.kubernetes.io/instance: mapper
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llama-mapper
        app.kubernetes.io/instance: mapper
      annotations:
        {}
    spec:
      serviceAccountName: mapper-llama-mapper
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
      containers:
        - name: app
          image: "ghcr.io/your-org/llama-mapper:0.1.0"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          env:
            - name: LLAMA_MAPPER_SERVING__HOST
              value: "0.0.0.0"
            - name: LLAMA_MAPPER_SERVING__PORT
              value: "8000"
            - name: LLAMA_MAPPER_SERVING__BACKEND
              value: "tgi"
            - name: LLAMA_MAPPER_MODEL__QUANTIZATION
              value: ""
            - name: LLAMA_MAPPER_TAXONOMY_PATH
              value: "/app/pillars-detectors/taxonomy.yaml"
            - name: LLAMA_MAPPER_FRAMEWORKS_PATH
              value: "/app/pillars-detectors/frameworks.yaml"
            - name: LLAMA_MAPPER_DETECTORS_PATH
              value: "/app/pillars-detectors"
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 15
            timeoutSeconds: 3
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 15
            timeoutSeconds: 3
            failureThreshold: 3
          volumeMounts:
            - name: pillars-detectors
              mountPath: /app/pillars-detectors
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
      volumes:
        - name: pillars-detectors
          emptyDir: {} # no config provided; endpoints relying on external storage
---
# Source: llama-mapper/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mapper-llama-mapper
  labels:
    helm.sh/chart: llama-mapper-0.1.0
    app.kubernetes.io/name: llama-mapper
    app.kubernetes.io/instance: mapper
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mapper-llama-mapper
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
---
# Source: llama-mapper/templates/prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: mapper-llama-mapper-alerts
  labels:
    helm.sh/chart: llama-mapper-0.1.0
    app.kubernetes.io/name: llama-mapper
    app.kubernetes.io/instance: mapper
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  groups:
  - name: llama-mapper.rules
    rules:
    - alert: MapperSchemaValidLow
      expr: mapper_schema_valid_percentage < 99.5
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Schema-valid percentage below threshold"
        description: "Schema validation percentage is below 99.5%."
        runbook_url: "https://REPLACE_WITH_REPO/docs/runbook/alert-runbooks.md#mapperschemavalidlow"

    - alert: MapperFallbackHigh
      expr: mapper_fallback_percentage > 10
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Fallback percentage above threshold"
        description: "Fallback mapping usage is above 10%."
        runbook_url: "https://REPLACE_WITH_REPO/docs/runbook/alert-runbooks.md#mapperfallbackhigh"

    - alert: MapperLatencyP95High
      expr: histogram_quantile(0.95, sum(rate(mapper_request_duration_seconds_bucket[5m])) by (le)) > 0.25
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "P95 latency above threshold"
        description: "P95 request latency is above the threshold for profile tgi."
        runbook_url: "https://REPLACE_WITH_REPO/docs/runbook/alert-runbooks.md#mapperlatencyp95high"

    - alert: Mapper5xxHigh
      expr: (sum(rate(mapper_requests_total{status="error"}[5m])) / sum(rate(mapper_requests_total[5m]))) * 100 > 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "5xx error rate high"
        description: "5xx (error) rate exceeded 1% over last 5 minutes."
        runbook_url: "https://REPLACE_WITH_REPO/docs/runbook/alert-runbooks.md#mapper5xxhigh"
---
# Source: llama-mapper/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: mapper-llama-mapper
  labels:
    helm.sh/chart: llama-mapper-0.1.0
    app.kubernetes.io/name: llama-mapper
    app.kubernetes.io/instance: mapper
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: llama-mapper
      app.kubernetes.io/instance: mapper
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
      scrapeTimeout: 5s
