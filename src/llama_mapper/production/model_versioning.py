"""
Production Hygiene: Model Versioning & Deployment

Version everything: {model_name}@{semver} + a one-page model card with datasets, metrics, known failure modes.
Canary new adapters behind a flag; promote when per-bucket KPIs beat the baseline (p95 latency, schema pass-rate, F1 on gold).
"""

import json
import logging
from dataclasses import asdict, dataclass
from datetime import datetime

# import semver  # External dependency - would need to be installed
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional


class DeploymentStatus(Enum):
    """Deployment status for model versions."""

    DEVELOPMENT = "development"
    STAGING = "staging"
    CANARY = "canary"
    PRODUCTION = "production"
    DEPRECATED = "deprecated"


class ModelType(Enum):
    """Model types for versioning."""

    MAPPER = "mapper"
    ANALYST = "analyst"


@dataclass
class ModelCard:
    """One-page model card with essential information."""

    model_name: str
    model_type: ModelType
    version: str
    created_at: str
    created_by: str

    # Training information
    training_datasets: List[str]
    training_steps: int
    training_epochs: int
    lora_config: Dict[str, Any]

    # Performance metrics
    performance_metrics: Dict[str, Any]
    golden_set_metrics: Optional[Dict[str, Any]] = None

    # Known issues
    known_failure_modes: Optional[List[str]] = None
    limitations: Optional[List[str]] = None

    # Deployment information
    deployment_status: DeploymentStatus = DeploymentStatus.DEVELOPMENT
    canary_percentage: float = 0.0
    baseline_version: Optional[str] = None

    # Metadata
    model_size: Optional[str] = None
    memory_requirements: Optional[str] = None
    inference_latency: Optional[float] = None
    throughput: Optional[float] = None

    def __post_init__(self):
        if self.known_failure_modes is None:
            self.known_failure_modes = []
        if self.limitations is None:
            self.limitations = []


@dataclass
class KPIMetrics:
    """KPI metrics for canary deployment evaluation."""

    p95_latency: float
    schema_pass_rate: float
    f1_score: float
    cache_hit_rate: float
    error_rate: float
    throughput: float
    timestamp: str

    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()


@dataclass
class CanaryConfig:
    """Configuration for canary deployment."""

    canary_percentage: float = 5.0  # 5% traffic to canary
    evaluation_duration: int = 3600  # 1 hour evaluation
    kpi_thresholds: Optional[Dict[str, float]] = None
    auto_promote: bool = False

    def __post_init__(self):
        if self.kpi_thresholds is None:
            self.kpi_thresholds = {
                "p95_latency_improvement": 0.1,  # 10% improvement
                "schema_pass_rate_min": 0.95,  # 95% minimum
                "f1_score_improvement": 0.02,  # 2% improvement
                "error_rate_max": 0.01,  # 1% maximum error rate
            }


class ModelVersionManager:
    """Manages model versions and deployments."""

    def __init__(
        self, models_dir: str = "models", registry_file: str = "model_registry.json"
    ):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(exist_ok=True)
        self.registry_file = self.models_dir / registry_file
        self.registry = self._load_registry()
        self.logger = logging.getLogger(__name__)

    def _load_registry(self) -> Dict[str, Any]:
        """Load model registry from file."""
        if self.registry_file.exists():
            with open(self.registry_file, "r") as f:
                return json.load(f)
        return {
            "models": {},
            "deployments": {},
            "last_updated": datetime.now().isoformat(),
        }

    def _save_registry(self) -> None:
        """Save model registry to file."""
        self.registry["last_updated"] = datetime.now().isoformat()
        with open(self.registry_file, "w") as f:
            json.dump(self.registry, f, indent=2)

    def create_model_version(
        self,
        model_name: str,
        model_type: ModelType,
        training_datasets: List[str],
        training_config: Dict[str, Any],
        performance_metrics: Dict[str, Any],
        golden_set_metrics: Dict[str, Any],
        created_by: str = "system",
    ) -> str:
        """Create a new model version."""

        # Generate version number
        version = self._generate_version_number(model_name, model_type)

        # Create model card
        model_card = ModelCard(
            model_name=model_name,
            model_type=model_type,
            version=version,
            created_at=datetime.now().isoformat(),
            created_by=created_by,
            training_datasets=training_datasets,
            training_steps=training_config.get("max_steps", 0),
            training_epochs=training_config.get("num_train_epochs", 0),
            lora_config=training_config.get("lora_config", {}),
            performance_metrics=performance_metrics,
            golden_set_metrics=golden_set_metrics,
            known_failure_modes=self._identify_failure_modes(performance_metrics),
            limitations=self._identify_limitations(performance_metrics),
            deployment_status=DeploymentStatus.DEVELOPMENT,
        )

        # Save model card
        model_dir = self.models_dir / f"{model_name}_{version}"
        model_dir.mkdir(exist_ok=True)

        model_card_file = model_dir / "model_card.json"
        with open(model_card_file, "w") as f:
            json.dump(asdict(model_card), f, indent=2)

        # Update registry
        model_key = f"{model_name}@{version}"
        self.registry["models"][model_key] = {
            "model_card": asdict(model_card),
            "model_dir": str(model_dir),
            "created_at": model_card.created_at,
        }

        self._save_registry()

        self.logger.info("Created model version: %s", model_key)
        return version

    def _generate_version_number(self, model_name: str, model_type: ModelType) -> str:
        """Generate semantic version number."""
        # Get existing versions for this model
        existing_versions = []
        for key in self.registry["models"].keys():
            if key.startswith(f"{model_name}@"):
                version_str = key.split("@")[1]
                try:
                    # existing_versions.append(semver.VersionInfo.parse(version_str))  # External dependency
                    existing_versions.append(version_str)  # Simplified for now
                except ValueError:
                    continue

        if not existing_versions:
            # First version
            return "1.0.0"

        # Get latest version
        latest_version = max(existing_versions)

        # Increment patch version
        new_version = latest_version.bump_patch()
        return str(new_version)

    def _identify_failure_modes(self, performance_metrics: Dict[str, Any]) -> List[str]:
        """Identify known failure modes from performance metrics."""
        failure_modes = []

        # Check for low performance
        if "f1_score" in performance_metrics and performance_metrics["f1_score"] < 0.8:
            failure_modes.append("Low F1 score on evaluation set")

        if (
            "schema_pass_rate" in performance_metrics
            and performance_metrics["schema_pass_rate"] < 0.9
        ):
            failure_modes.append("Schema validation failures")

        if (
            "latency_p95" in performance_metrics
            and performance_metrics["latency_p95"] > 200
        ):
            failure_modes.append("High latency (>200ms p95)")

        # Check for specific taxonomy issues
        if "per_class_metrics" in performance_metrics:
            per_class = performance_metrics["per_class_metrics"]
            for class_name, metrics in per_class.items():
                if metrics.get("f1", 0) < 0.7:
                    failure_modes.append(f"Poor performance on {class_name}")

        return failure_modes

    def _identify_limitations(self, performance_metrics: Dict[str, Any]) -> List[str]:
        """Identify model limitations from performance metrics."""
        limitations = []

        # Check taxonomy coverage
        if "taxonomy_coverage" in performance_metrics:
            coverage = performance_metrics["taxonomy_coverage"]
            if coverage.get("total_branches", 0) < 20:
                limitations.append("Limited taxonomy coverage")

        # Check confidence distribution
        if "confidence_statistics" in performance_metrics:
            conf_stats = performance_metrics["confidence_statistics"]
            if conf_stats.get("mean", 0) < 0.8:
                limitations.append("Low average confidence scores")

        return limitations

    def deploy_canary(
        self,
        model_name: str,
        version: str,
        canary_config: CanaryConfig,
        baseline_version: Optional[str] = None,
    ) -> bool:
        """Deploy model version as canary."""

        model_key = f"{model_name}@{version}"
        if model_key not in self.registry["models"]:
            self.logger.error("Model %s not found in registry", model_key)
            return False

        # Update model card
        model_info = self.registry["models"][model_key]
        model_card = ModelCard(**model_info["model_card"])
        model_card.deployment_status = DeploymentStatus.CANARY
        model_card.canary_percentage = canary_config.canary_percentage
        model_card.baseline_version = baseline_version

        # Save updated model card
        model_dir = Path(model_info["model_dir"])
        model_card_file = model_dir / "model_card.json"
        with open(model_card_file, "w") as f:
            json.dump(asdict(model_card), f, indent=2)

        # Update registry
        model_info["model_card"] = asdict(model_card)
        self.registry["deployments"][model_key] = {
            "status": "canary",
            "canary_config": asdict(canary_config),
            "deployed_at": datetime.now().isoformat(),
            "baseline_version": baseline_version,
        }

        self._save_registry()

        self.logger.info(
            "Deployed canary: %s (%s%% traffic)",
            model_key,
            canary_config.canary_percentage,
        )
        return True

    def promote_to_production(
        self, model_name: str, version: str, kpi_metrics: KPIMetrics
    ) -> bool:
        """Promote canary to production based on KPI metrics."""

        model_key = f"{model_name}@{version}"
        if model_key not in self.registry["models"]:
            self.logger.error("Model %s not found in registry", model_key)
            return False

        # Check if model is in canary
        if model_key not in self.registry["deployments"]:
            self.logger.error("Model %s not deployed as canary", model_key)
            return False

        deployment_info = self.registry["deployments"][model_key]
        if deployment_info["status"] != "canary":
            self.logger.error("Model %s is not in canary status", model_key)
            return False

        # Evaluate KPI metrics
        canary_config = CanaryConfig(**deployment_info["canary_config"])
        if not self._evaluate_kpi_metrics(kpi_metrics, canary_config):
            self.logger.warning(
                f"KPI metrics for {model_key} do not meet promotion criteria"
            )
            return False

        # Promote to production
        model_info = self.registry["models"][model_key]
        model_card = ModelCard(**model_info["model_card"])
        model_card.deployment_status = DeploymentStatus.PRODUCTION

        # Save updated model card
        model_dir = Path(model_info["model_dir"])
        model_card_file = model_dir / "model_card.json"
        with open(model_card_file, "w") as f:
            json.dump(asdict(model_card), f, indent=2)

        # Update registry
        model_info["model_card"] = asdict(model_card)
        deployment_info["status"] = "production"
        deployment_info["promoted_at"] = datetime.now().isoformat()
        deployment_info["kpi_metrics"] = asdict(kpi_metrics)

        self._save_registry()

        self.logger.info("Promoted to production: %s", model_key)
        return True

    def _evaluate_kpi_metrics(
        self, kpi_metrics: KPIMetrics, canary_config: CanaryConfig
    ) -> bool:
        """Evaluate KPI metrics against canary configuration thresholds."""
        thresholds = canary_config.kpi_thresholds
        if not thresholds:
            return False

        # Check schema pass rate
        if kpi_metrics.schema_pass_rate < thresholds["schema_pass_rate_min"]:
            return False

        # Check error rate
        if kpi_metrics.error_rate > thresholds["error_rate_max"]:
            return False

        # Check latency improvement (if baseline available)
        # This would require baseline metrics comparison

        # Check F1 score improvement (if baseline available)
        # This would require baseline metrics comparison

        return True

    def get_model_info(self, model_name: str, version: str) -> Optional[Dict[str, Any]]:
        """Get model information."""
        model_key = f"{model_name}@{version}"
        return self.registry["models"].get(model_key)

    def list_model_versions(self, model_name: str) -> List[str]:
        """List all versions of a model."""
        versions = []
        for key in self.registry["models"].keys():
            if key.startswith(f"{model_name}@"):
                version = key.split("@")[1]
                versions.append(version)

        # Sort by semantic version
        try:
            # versions.sort(key=lambda v: semver.VersionInfo.parse(v), reverse=True)  # External dependency
            versions.sort(reverse=True)  # Simplified for now
        except ValueError:
            versions.sort(reverse=True)

        return versions

    def get_deployment_status(self, model_name: str, version: str) -> Optional[str]:
        """Get deployment status of a model version."""
        model_key = f"{model_name}@{version}"
        if model_key in self.registry["deployments"]:
            return self.registry["deployments"][model_key]["status"]
        return None

    def generate_model_card_markdown(self, model_name: str, version: str) -> str:
        """Generate markdown model card."""
        model_info = self.get_model_info(model_name, version)
        if not model_info:
            return f"Model {model_name}@{version} not found"

        model_card = ModelCard(**model_info["model_card"])

        markdown = f"""# Model Card: {model_card.model_name}@{model_card.version}

## Basic Information
- **Model Type**: {model_card.model_type.value}
- **Version**: {model_card.version}
- **Created**: {model_card.created_at}
- **Created By**: {model_card.created_by}
- **Status**: {model_card.deployment_status.value}

## Training Information
- **Training Steps**: {model_card.training_steps}
- **Training Epochs**: {model_card.training_epochs}
- **Datasets**: {', '.join(model_card.training_datasets)}

## LoRA Configuration
```json
{json.dumps(model_card.lora_config, indent=2)}
```

## Performance Metrics
```json
{json.dumps(model_card.performance_metrics, indent=2)}
```

## Golden Set Metrics
```json
{json.dumps(model_card.golden_set_metrics, indent=2)}
```

## Known Failure Modes
{chr(10).join(f"- {mode}" for mode in (model_card.known_failure_modes or []))}

## Limitations
{chr(10).join(f"- {limitation}" for limitation in (model_card.limitations or []))}

## Deployment Information
- **Status**: {model_card.deployment_status.value}
- **Canary Percentage**: {model_card.canary_percentage}%
- **Baseline Version**: {model_card.baseline_version or "N/A"}
"""

        return markdown


class CanaryEvaluator:
    """Evaluates canary deployments against baseline."""

    def __init__(self, version_manager: ModelVersionManager):
        self.version_manager = version_manager
        self.evaluation_results = {}

    def evaluate_canary(
        self,
        model_name: str,
        canary_version: str,
        baseline_version: str,
        evaluation_duration: int = 3600,
    ) -> Dict[str, Any]:
        """Evaluate canary against baseline."""

        # This would collect real metrics from production
        # For now, we'll simulate the evaluation

        evaluation_result = {
            "canary_version": canary_version,
            "baseline_version": baseline_version,
            "evaluation_duration": evaluation_duration,
            "kpi_comparison": {
                "p95_latency": {
                    "canary": 85.0,  # ms
                    "baseline": 95.0,  # ms
                    "improvement": 0.105,  # 10.5% improvement
                },
                "schema_pass_rate": {
                    "canary": 0.97,
                    "baseline": 0.95,
                    "improvement": 0.021,  # 2.1% improvement
                },
                "f1_score": {
                    "canary": 0.92,
                    "baseline": 0.90,
                    "improvement": 0.022,  # 2.2% improvement
                },
                "error_rate": {
                    "canary": 0.008,
                    "baseline": 0.012,
                    "improvement": 0.333,  # 33.3% improvement
                },
            },
            "recommendation": "promote",
            "confidence": 0.95,
        }

        self.evaluation_results[f"{model_name}@{canary_version}"] = evaluation_result
        return evaluation_result

    def should_promote_canary(self, model_name: str, canary_version: str) -> bool:
        """Determine if canary should be promoted."""
        evaluation_key = f"{model_name}@{canary_version}"
        if evaluation_key not in self.evaluation_results:
            return False

        evaluation = self.evaluation_results[evaluation_key]
        return (
            evaluation["recommendation"] == "promote" and evaluation["confidence"] > 0.9
        )


# Example usage and testing
if __name__ == "__main__":
    # Create version manager
    version_manager = ModelVersionManager()

    # Create a sample model version
    training_datasets = ["ai4privacy/pii-masking-43k", "nguha/legalbench"]
    training_config = {
        "max_steps": 1000,
        "num_train_epochs": 3,
        "lora_config": {"r": 128, "alpha": 256, "dropout": 0.1},
    }
    performance_metrics = {
        "f1_score": 0.92,
        "schema_pass_rate": 0.97,
        "latency_p95": 85.0,
        "per_class_metrics": {
            "PII.Contact.Email": {"f1": 0.95},
            "PII.Contact.Phone": {"f1": 0.89},
        },
    }
    golden_set_metrics = {
        "overall_accuracy": 0.94,
        "per_branch_accuracy": {"PII.Contact.Email": 0.96, "PII.Contact.Phone": 0.91},
    }

    version = version_manager.create_model_version(
        "llama-mapper",
        ModelType.MAPPER,
        training_datasets,
        training_config,
        performance_metrics,
        golden_set_metrics,
    )

    print(f"Created model version: {version}")

    # Deploy as canary
    canary_config = CanaryConfig(canary_percentage=5.0)
    success = version_manager.deploy_canary("llama-mapper", version, canary_config)
    print(f"Deployed canary: {success}")

    # Generate model card
    model_card_md = version_manager.generate_model_card_markdown(
        "llama-mapper", version
    )
    print(f"Model card generated: {len(model_card_md)} characters")

    print(f"\nðŸŽ‰ Production Hygiene System Ready!")
    print(f"  - Model versioning: llama-mapper@{version}")
    print(f"  - Model cards: One-page with datasets, metrics, failure modes")
    print(f"  - Canary deployments: KPI-based promotion")
    print(f"  - Registry: Centralized model management")
