[tool:pytest]
# Unified pytest configuration for Llama Mapper multi-service testing

# Test discovery patterns
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Minimum version requirement
minversion = 7.0

# Addopts for consistent test execution
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --cov=src/llama_mapper
    --cov=detector-orchestration/src
    --cov-report=term-missing
    --cov-report=html:tests/coverage/html
    --cov-report=xml:tests/coverage/coverage.xml
    --cov-fail-under=85
    --maxfail=5
    --durations=10
    --junitxml=tests/coverage/junit.xml

# Asyncio configuration
asyncio_mode = auto

# Test markers for organizing tests
markers =
    unit: Unit tests for individual components
    integration: Integration tests across components within a service
    cross_service: Integration tests spanning multiple services
    performance: Performance and load tests
    security: Security and privacy tests
    chaos: Chaos engineering and fault tolerance tests
    e2e: End-to-end workflow tests
    slow: Slow running tests (> 30 seconds)
    core_mapper: Tests specific to Core Mapper service
    detector_orchestration: Tests specific to Detector Orchestration service
    analysis_service: Tests specific to Analysis service
    database: Tests requiring database setup
    redis: Tests requiring Redis setup
    docker: Tests requiring Docker containers
    golden: Golden dataset regression tests
    mutation: Mutation testing scenarios
    contract: API contract validation tests
    load: Load testing scenarios
    stress: Stress testing scenarios
    endurance: Endurance testing scenarios
    smoke: Smoke tests for basic functionality
    regression: Regression tests for known issues

# Test filtering options
# Run fast tests by default (exclude slow, performance, chaos)
testpaths_fast = tests/unit tests/integration
testpaths_full = tests

# Logging configuration for tests
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_file_date_format = %Y-%m-%d %H:%M:%S

# Timeout configuration
timeout = 300
timeout_method = thread

# Warnings configuration
filterwarnings =
    error
    ignore::UserWarning
    ignore::DeprecationWarning:pkg_resources
    ignore::PendingDeprecationWarning
    ignore:.*pytest-asyncio*:PytestUnraisableExceptionWarning

# Custom pytest plugins
plugins = 
    pytest_asyncio
    pytest-cov
    pytest-timeout
    pytest-mock
    pytest-html
    pytest-xdist

# Parallel execution configuration
# Use -n auto for automatic CPU detection or -n <number> for specific worker count
# Example: pytest -n auto tests/

# Environment variables for tests
env =
    TESTING = true
    LOG_LEVEL = DEBUG
    PYTHONPATH = src:detector-orchestration/src
    PRIVACY_MODE = true

# Test collection configuration
collect_ignore = [
    "setup.py",
    "docs/",
    "examples/",
    "scripts/",
    "notebooks/",
    "checkpoints/",
    "htmlcov/",
    "venv/",
    ".git/",
    ".kiro/",
    "node_modules/",
    "cursor-chat-browser/"
]

# Test execution order
# Default is by modification time, can be overridden with --tb=line
usefixtures = setup_test_environment