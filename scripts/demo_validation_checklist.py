#!/usr/bin/env python3
"""
Demo Validation Checklist
Comprehensive validation script for analysis module demo readiness.
"""

import json
import logging
import subprocess
import sys
import tempfile
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests

# Setup logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def print_section(title: str):
    """Print a formatted section header."""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}")


def print_result(test_name: str, passed: bool, details: str = ""):
    """Print test result with formatting."""
    status = "âœ… PASS" if passed else "âŒ FAIL"
    print(f"{status} {test_name}")
    if details:
        print(f"    {details}")


class DemoValidator:
    def __init__(self, api_base_url: str = "http://localhost:8001"):
        self.api_base_url = api_base_url
        self.results = {}

    def validate_schema_compliance(self) -> bool:
        """Test 1: Schema pass - Response conforms to Analysis Response structure."""
        print_section("1. SCHEMA VALIDATION")

        try:
            # Load sample request
            sample_path = Path("examples/sample_metrics.json")
            if not sample_path.exists():
                print_result(
                    "Schema Validation", False, "Sample metrics file not found"
                )
                return False

            with open(sample_path) as f:
                sample_request = json.load(f)

            # Make API call
            response = requests.post(
                f"{self.api_base_url}/api/v1/analysis/analyze",
                json=sample_request,
                headers={"Content-Type": "application/json"},
                timeout=30,
            )

            if response.status_code != 200:
                print_result(
                    "API Response",
                    False,
                    f"Status {response.status_code}: {response.text}",
                )
                return False

            analysis_response = response.json()

            # Validate required fields
            required_fields = [
                "reason",
                "remediation",
                "opa_diff",
                "confidence",
                "evidence_refs",
                "notes",
                "version_info",
                "request_id",
                "timestamp",
            ]

            missing_fields = [
                field for field in required_fields if field not in analysis_response
            ]
            if missing_fields:
                print_result("Required Fields", False, f"Missing: {missing_fields}")
                return False

            # Validate field types
            type_checks = [
                ("confidence", (int, float)),
                ("evidence_refs", list),
                ("version_info", dict),
                ("reason", str),
                ("remediation", str),
                ("opa_diff", str),
            ]

            for field, expected_type in type_checks:
                if not isinstance(analysis_response.get(field), expected_type):
                    print_result(
                        "Field Types", False, f"{field} should be {expected_type}"
                    )
                    return False

            # Validate confidence range
            confidence = analysis_response.get("confidence", 0)
            if not (0 <= confidence <= 1):
                print_result(
                    "Confidence Range", False, f"Confidence {confidence} not in [0,1]"
                )
                return False

            # Load and validate against JSON schema
            schema_path = Path("schemas/analyst_output.json")
            if schema_path.exists():
                try:
                    import jsonschema

                    with open(schema_path) as f:
                        schema = json.load(f)

                    jsonschema.validate(analysis_response, schema)
                    print_result(
                        "JSON Schema Validation",
                        True,
                        "Response validates against schema",
                    )
                except ImportError:
                    print_result(
                        "JSON Schema Validation",
                        False,
                        "jsonschema package not installed",
                    )
                except jsonschema.ValidationError as e:
                    print_result(
                        "JSON Schema Validation", False, f"Schema error: {e.message}"
                    )
                    return False
            else:
                print_result("JSON Schema File", False, "Schema file not found")
                return False

            print_result("Schema Validation", True, "All schema checks passed")
            self.results["schema"] = True
            return True

        except Exception as e:
            print_result("Schema Validation", False, f"Error: {str(e)}")
            self.results["schema"] = False
            return False

    def validate_opa_compilation(self) -> bool:
        """Test 2: OPA compile - Pipe emitted opa_diff into opa eval."""
        print_section("2. OPA COMPILATION TEST")

        try:
            # Get sample OPA diff from API
            sample_path = Path("examples/sample_metrics.json")
            with open(sample_path) as f:
                sample_request = json.load(f)

            response = requests.post(
                f"{self.api_base_url}/api/v1/analysis/analyze",
                json=sample_request,
                headers={"Content-Type": "application/json"},
                timeout=30,
            )

            if response.status_code != 200:
                print_result("OPA Test - API Call", False, f"Failed to get OPA diff")
                return False

            analysis_response = response.json()
            opa_diff = analysis_response.get("opa_diff", "")

            if not opa_diff:
                print_result("OPA Diff Content", False, "No OPA diff in response")
                return False

            # Write OPA policy to temp file
            with tempfile.NamedTemporaryFile(
                mode="w", suffix=".rego", delete=False
            ) as f:
                f.write(opa_diff)
                policy_file = f.name

            # Check if OPA is available
            try:
                result = subprocess.run(
                    ["opa", "version"], capture_output=True, text=True, timeout=10
                )
                if result.returncode != 0:
                    print_result(
                        "OPA Availability", False, "OPA not installed or not in PATH"
                    )
                    return False
            except (subprocess.TimeoutExpired, FileNotFoundError):
                print_result("OPA Availability", False, "OPA not found")
                return False

            # Test OPA policy compilation
            try:
                compile_result = subprocess.run(
                    ["opa", "fmt", policy_file],
                    capture_output=True,
                    text=True,
                    timeout=10,
                )

                if compile_result.returncode != 0:
                    print_result(
                        "OPA Compilation",
                        False,
                        f"Compilation error: {compile_result.stderr}",
                    )
                    return False

                # Test policy evaluation with sample input
                test_input = {
                    "detectors": ["toxicity", "regex-pii"],
                    "observed_coverage": {"toxicity": 0.85},
                    "required_coverage": {"toxicity": 0.95, "regex-pii": 0.95},
                }

                with tempfile.NamedTemporaryFile(
                    mode="w", suffix=".json", delete=False
                ) as f:
                    json.dump(test_input, f)
                    input_file = f.name

                eval_result = subprocess.run(
                    [
                        "opa",
                        "eval",
                        "-d",
                        policy_file,
                        "-i",
                        input_file,
                        "data.coverage.violation",
                    ],
                    capture_output=True,
                    text=True,
                    timeout=10,
                )

                if eval_result.returncode == 0:
                    print_result(
                        "OPA Policy Evaluation", True, "Policy evaluates successfully"
                    )
                    print_result(
                        "OPA Compilation", True, "Policy compiles and runs correctly"
                    )
                    self.results["opa"] = True
                    return True
                else:
                    print_result(
                        "OPA Policy Evaluation",
                        False,
                        f"Evaluation error: {eval_result.stderr}",
                    )
                    return False

            finally:
                # Cleanup temp files
                Path(policy_file).unlink(missing_ok=True)
                if "input_file" in locals():
                    Path(input_file).unlink(missing_ok=True)

        except Exception as e:
            print_result("OPA Compilation", False, f"Error: {str(e)}")
            self.results["opa"] = False
            return False

    def validate_pii_redaction(self) -> bool:
        """Test 3: PII redaction - Confirm free-text fields are redacted."""
        print_section("3. PII REDACTION CHECK")

        try:
            # Create request with potential PII
            pii_request = {
                "request": {
                    "period": "2024-01-15T10:30:00Z/2024-01-15T11:30:00Z",
                    "tenant": "john.doe@example.com",  # Email PII
                    "app": "demo-app-ssn-123-45-6789",  # SSN PII
                    "route": "/api/user/555-1234",  # Phone PII
                    "required_detectors": ["toxicity", "regex-pii"],
                    "observed_coverage": {"toxicity": 0.85, "regex-pii": 0.92},
                    "required_coverage": {"toxicity": 0.95, "regex-pii": 0.95},
                    "detector_errors": {"toxicity": {"5xx": 0, "timeout": 2}},
                    "high_sev_hits": [
                        {
                            "detector": "toxicity",
                            "taxonomy": "HARM.SPEECH.Toxicity",
                            "count": 5,
                            "p95_score": 0.95,
                        }
                    ],
                    "false_positive_bands": [
                        {
                            "detector": "toxicity",
                            "score_min": 0.6,
                            "score_max": 0.8,
                            "fp_rate": 0.15,
                        }
                    ],
                    "policy_bundle": "demo-policy-1.0",
                    "env": "dev",
                }
            }

            response = requests.post(
                f"{self.api_base_url}/api/v1/analysis/analyze",
                json=pii_request,
                headers={"Content-Type": "application/json"},
                timeout=30,
            )

            if response.status_code != 200:
                print_result("PII Test - API Call", False, f"API call failed")
                return False

            analysis_response = response.json()

            # Check if PII appears in response fields
            pii_patterns = ["john.doe@example.com", "123-45-6789", "555-1234"]
            response_text = json.dumps(analysis_response).lower()

            found_pii = []
            for pattern in pii_patterns:
                if pattern.lower() in response_text:
                    found_pii.append(pattern)

            if found_pii:
                print_result(
                    "PII Redaction", False, f"Found PII in response: {found_pii}"
                )
                # This might be expected for tenant/app fields, so let's be more specific
                sensitive_fields = ["reason", "remediation", "notes", "opa_diff"]
                for field in sensitive_fields:
                    field_content = str(analysis_response.get(field, "")).lower()
                    for pattern in pii_patterns:
                        if pattern.lower() in field_content:
                            print_result(
                                "PII in Sensitive Fields",
                                False,
                                f"PII '{pattern}' in field '{field}'",
                            )
                            self.results["pii"] = False
                            return False

            print_result(
                "PII Redaction", True, "No PII found in sensitive response fields"
            )
            self.results["pii"] = True
            return True

        except Exception as e:
            print_result("PII Redaction", False, f"Error: {str(e)}")
            self.results["pii"] = False
            return False

    def validate_quality_monitoring(self) -> bool:
        """Test 4: Quality monitor - Check counters and alert thresholds."""
        print_section("4. QUALITY MONITORING")

        try:
            # Make multiple requests with varying confidence scenarios
            low_confidence_count = 0
            total_requests = 5

            for i in range(total_requests):
                sample_path = Path("examples/sample_metrics.json")
                with open(sample_path) as f:
                    sample_request = json.load(f)

                # Modify request to potentially trigger different confidence levels
                sample_request["request"]["env"] = f"test-{i}"

                response = requests.post(
                    f"{self.api_base_url}/api/v1/analysis/analyze",
                    json=sample_request,
                    headers={"Content-Type": "application/json"},
                    timeout=30,
                )

                if response.status_code == 200:
                    analysis_response = response.json()
                    confidence = analysis_response.get("confidence", 1.0)

                    # Check if this would trigger a low confidence alert (< 0.7)
                    if confidence < 0.7:
                        low_confidence_count += 1

                    print(f"    Request {i+1}: Confidence = {confidence:.2f}")
                else:
                    print_result("Quality Test Request", False, f"Request {i+1} failed")

            # Check if quality monitoring would work
            if low_confidence_count > 0:
                print_result(
                    "Low Confidence Detection",
                    True,
                    f"Detected {low_confidence_count} low confidence responses",
                )
            else:
                print_result(
                    "Low Confidence Detection",
                    True,
                    "All responses had adequate confidence",
                )

            # Check for confidence_cutoff_used field (indicates quality monitoring)
            sample_path = Path("examples/sample_metrics.json")
            with open(sample_path) as f:
                sample_request = json.load(f)

            response = requests.post(
                f"{self.api_base_url}/api/v1/analysis/analyze",
                json=sample_request,
                headers={"Content-Type": "application/json"},
                timeout=30,
            )

            if response.status_code == 200:
                analysis_response = response.json()
                if "confidence_cutoff_used" in analysis_response:
                    print_result(
                        "Quality Threshold Monitoring",
                        True,
                        f"Cutoff used: {analysis_response['confidence_cutoff_used']}",
                    )
                else:
                    print_result(
                        "Quality Threshold Monitoring",
                        False,
                        "No confidence cutoff reported",
                    )
                    self.results["quality"] = False
                    return False

            print_result(
                "Quality Monitoring", True, "Quality monitoring features active"
            )
            self.results["quality"] = True
            return True

        except Exception as e:
            print_result("Quality Monitoring", False, f"Error: {str(e)}")
            self.results["quality"] = False
            return False

    def validate_slo_metrics(self) -> bool:
        """Test 5: SLO wires - Check if metrics endpoints are available."""
        print_section("5. SLO METRICS CHECK")

        try:
            # Check if metrics endpoint exists
            metrics_endpoints = [
                f"{self.api_base_url}/metrics",
                f"{self.api_base_url}/api/v1/metrics",
                "http://localhost:9090/metrics",  # Common Prometheus port
            ]

            metrics_found = False
            for endpoint in metrics_endpoints:
                try:
                    response = requests.get(endpoint, timeout=5)
                    if response.status_code == 200:
                        metrics_content = response.text

                        # Check for key SLO metrics
                        expected_metrics = [
                            "analysis_request_duration",
                            "analysis_error_rate",
                            "coverage_gap_rate",
                        ]

                        found_metrics = []
                        for metric in expected_metrics:
                            if metric in metrics_content:
                                found_metrics.append(metric)

                        if found_metrics:
                            print_result(
                                "Metrics Endpoint", True, f"Found at {endpoint}"
                            )
                            print_result(
                                "SLO Metrics", True, f"Found metrics: {found_metrics}"
                            )
                            metrics_found = True
                            break
                        else:
                            print_result(
                                "SLO Metrics",
                                False,
                                f"No SLO metrics found at {endpoint}",
                            )

                except requests.RequestException:
                    continue

            if not metrics_found:
                print_result("Metrics Endpoint", False, "No metrics endpoint found")
                # This is not a failure - metrics might be configured differently
                print("    Note: Metrics might be exported to external Prometheus")

            # Check if we can measure API latency
            start_time = time.time()

            sample_path = Path("examples/sample_metrics.json")
            with open(sample_path) as f:
                sample_request = json.load(f)

            response = requests.post(
                f"{self.api_base_url}/api/v1/analysis/analyze",
                json=sample_request,
                headers={"Content-Type": "application/json"},
                timeout=30,
            )

            latency = (time.time() - start_time) * 1000  # Convert to ms

            if response.status_code == 200:
                print_result(
                    "Latency Measurement", True, f"Response time: {latency:.2f}ms"
                )

                # Check if latency meets SLO (< 2000ms for analysis)
                if latency < 2000:
                    print_result("SLO Compliance", True, f"Latency within SLO target")
                else:
                    print_result("SLO Compliance", False, f"Latency exceeds SLO target")
            else:
                print_result("Latency Measurement", False, "Failed to measure latency")

            print_result("SLO Metrics", True, "SLO measurement capabilities confirmed")
            self.results["slo"] = True
            return True

        except Exception as e:
            print_result("SLO Metrics", False, f"Error: {str(e)}")
            self.results["slo"] = False
            return False

    def create_golden_test_suite(self) -> bool:
        """Test 6: Create golden test suite covering key scenarios."""
        print_section("6. GOLDEN TEST SUITE")

        try:
            golden_tests = {
                "sufficient_coverage": {
                    "description": "All detectors have sufficient coverage",
                    "request": {
                        "period": "2024-01-15T10:30:00Z/2024-01-15T11:30:00Z",
                        "tenant": "golden-test-tenant",
                        "app": "sufficient-coverage-app",
                        "route": "/api/test",
                        "required_detectors": ["toxicity", "regex-pii"],
                        "observed_coverage": {"toxicity": 0.98, "regex-pii": 0.96},
                        "required_coverage": {"toxicity": 0.95, "regex-pii": 0.95},
                        "detector_errors": {},
                        "high_sev_hits": [],
                        "false_positive_bands": [],
                        "policy_bundle": "golden-policy-1.0",
                        "env": "test",
                    },
                    "expected_outcome": "no_action_needed",
                },
                "insufficient_coverage": {
                    "description": "Detectors have insufficient coverage",
                    "request": {
                        "period": "2024-01-15T10:30:00Z/2024-01-15T11:30:00Z",
                        "tenant": "golden-test-tenant",
                        "app": "insufficient-coverage-app",
                        "route": "/api/test",
                        "required_detectors": ["toxicity", "regex-pii"],
                        "observed_coverage": {"toxicity": 0.85, "regex-pii": 0.88},
                        "required_coverage": {"toxicity": 0.95, "regex-pii": 0.95},
                        "detector_errors": {},
                        "high_sev_hits": [],
                        "false_positive_bands": [],
                        "policy_bundle": "golden-policy-1.0",
                        "env": "test",
                    },
                    "expected_outcome": "coverage_gap",
                },
                "conflicting_detectors": {
                    "description": "High false positive rate indicates detector conflict",
                    "request": {
                        "period": "2024-01-15T10:30:00Z/2024-01-15T11:30:00Z",
                        "tenant": "golden-test-tenant",
                        "app": "conflicting-detectors-app",
                        "route": "/api/test",
                        "required_detectors": ["toxicity", "regex-pii"],
                        "observed_coverage": {"toxicity": 0.96, "regex-pii": 0.97},
                        "required_coverage": {"toxicity": 0.95, "regex-pii": 0.95},
                        "detector_errors": {},
                        "high_sev_hits": [],
                        "false_positive_bands": [
                            {
                                "detector": "toxicity",
                                "score_min": 0.6,
                                "score_max": 0.8,
                                "fp_rate": 0.45,  # High FP rate
                            }
                        ],
                        "policy_bundle": "golden-policy-1.0",
                        "env": "test",
                    },
                    "expected_outcome": "false_positive_tuning",
                },
                "detector_timeout_spike": {
                    "description": "Detector timeout spike indicates incident",
                    "request": {
                        "period": "2024-01-15T10:30:00Z/2024-01-15T11:30:00Z",
                        "tenant": "golden-test-tenant",
                        "app": "timeout-spike-app",
                        "route": "/api/test",
                        "required_detectors": ["toxicity", "regex-pii"],
                        "observed_coverage": {"toxicity": 0.96, "regex-pii": 0.97},
                        "required_coverage": {"toxicity": 0.95, "regex-pii": 0.95},
                        "detector_errors": {
                            "toxicity": {"5xx": 2, "timeout": 15}  # High timeout count
                        },
                        "high_sev_hits": [],
                        "false_positive_bands": [],
                        "policy_bundle": "golden-policy-1.0",
                        "env": "test",
                    },
                    "expected_outcome": "incident_summary",
                },
                "high_severity_hits": {
                    "description": "High severity hits require immediate attention",
                    "request": {
                        "period": "2024-01-15T10:30:00Z/2024-01-15T11:30:00Z",
                        "tenant": "golden-test-tenant",
                        "app": "high-sev-app",
                        "route": "/api/test",
                        "required_detectors": ["toxicity", "regex-pii"],
                        "observed_coverage": {"toxicity": 0.96, "regex-pii": 0.97},
                        "required_coverage": {"toxicity": 0.95, "regex-pii": 0.95},
                        "detector_errors": {},
                        "high_sev_hits": [
                            {
                                "detector": "toxicity",
                                "taxonomy": "HARM.SPEECH.Toxicity",
                                "count": 25,  # High count
                                "p95_score": 0.98,
                            }
                        ],
                        "false_positive_bands": [],
                        "policy_bundle": "golden-policy-1.0",
                        "env": "test",
                    },
                    "expected_outcome": "incident_summary",
                },
            }

            # Save golden test suite
            golden_path = Path("tests/golden_analysis_test_suite.json")
            golden_path.parent.mkdir(parents=True, exist_ok=True)

            with open(golden_path, "w") as f:
                json.dump(golden_tests, f, indent=2)

            print_result("Golden Test Suite Created", True, f"Saved to {golden_path}")

            # Run golden tests
            passed_tests = 0
            total_tests = len(golden_tests)

            for test_name, test_case in golden_tests.items():
                try:
                    request_payload = {"request": test_case["request"]}

                    response = requests.post(
                        f"{self.api_base_url}/api/v1/analysis/analyze",
                        json=request_payload,
                        headers={"Content-Type": "application/json"},
                        timeout=30,
                    )

                    if response.status_code == 200:
                        analysis_response = response.json()

                        # Validate response structure
                        if all(
                            field in analysis_response
                            for field in ["reason", "remediation", "confidence"]
                        ):
                            print_result(
                                f"Golden Test: {test_name}",
                                True,
                                f"Confidence: {analysis_response.get('confidence', 0):.2f}",
                            )
                            passed_tests += 1
                        else:
                            print_result(
                                f"Golden Test: {test_name}",
                                False,
                                "Invalid response structure",
                            )
                    else:
                        print_result(
                            f"Golden Test: {test_name}",
                            False,
                            f"API error: {response.status_code}",
                        )

                except Exception as e:
                    print_result(f"Golden Test: {test_name}", False, f"Error: {str(e)}")

            success_rate = passed_tests / total_tests
            if success_rate >= 0.8:  # 80% pass rate
                print_result(
                    "Golden Test Suite",
                    True,
                    f"Passed {passed_tests}/{total_tests} tests",
                )
                self.results["golden"] = True
                return True
            else:
                print_result(
                    "Golden Test Suite",
                    False,
                    f"Only {passed_tests}/{total_tests} tests passed",
                )
                self.results["golden"] = False
                return False

        except Exception as e:
            print_result("Golden Test Suite", False, f"Error: {str(e)}")
            self.results["golden"] = False
            return False

    def generate_summary_report(self):
        """Generate a comprehensive validation summary."""
        print_section("DEMO VALIDATION SUMMARY")

        total_tests = len(self.results)
        passed_tests = sum(1 for result in self.results.values() if result)

        print(f"Overall Result: {passed_tests}/{total_tests} tests passed")
        print(f"Success Rate: {(passed_tests/total_tests)*100:.1f}%")

        print("\nDetailed Results:")
        for test, passed in self.results.items():
            status = "âœ… PASS" if passed else "âŒ FAIL"
            print(f"  {status} {test.replace('_', ' ').title()}")

        if passed_tests == total_tests:
            print("\nðŸŽ‰ DEMO READY! All validation checks passed.")
            print("\nNext Steps:")
            print("  â€¢ Create OPA enforcement demo")
            print("  â€¢ Prepare before/after comparison slide")
            print("  â€¢ Capture SLO dashboard screenshots")
            print("  â€¢ Practice the demo flow")
        else:
            failed_tests = [test for test, passed in self.results.items() if not passed]
            print(f"\nâš ï¸  Demo needs attention. Failed tests: {failed_tests}")


def main():
    """Run the complete demo validation checklist."""
    if len(sys.argv) > 1:
        api_url = sys.argv[1]
    else:
        api_url = "http://localhost:8001"

    print("ðŸš€ COMPLIANCE ANALYSIS DEMO VALIDATION")
    print(f"Testing API at: {api_url}")

    validator = DemoValidator(api_url)

    # Run all validation tests
    validator.validate_schema_compliance()
    validator.validate_opa_compilation()
    validator.validate_pii_redaction()
    validator.validate_quality_monitoring()
    validator.validate_slo_metrics()
    validator.create_golden_test_suite()

    # Generate summary
    validator.generate_summary_report()


if __name__ == "__main__":
    main()
