#!/usr/bin/env python3
"""
Standalone monitoring demo that works without Docker.
Shows the key metrics and alerts in action.
"""

import json
import random
import threading
import time
from concurrent.futures import ThreadPoolExecutor

import requests

API_BASE = "http://localhost:8001"
TENANTS = ["acme-corp", "global-bank", "healthcare-plus", "fintech-startup"]


def check_metrics_endpoint():
    """Check if metrics endpoint is working."""
    try:
        response = requests.get(f"{API_BASE}/metrics", timeout=5)
        if response.status_code == 200:
            print("âœ… Metrics endpoint working")

            # Look for our custom metrics
            metrics_text = response.text
            custom_metrics = [
                "analysis_requests_total",
                "analysis_request_duration_seconds",
                "analysis_confidence_score",
                "coverage_gap_rate",
            ]

            found_metrics = []
            for metric in custom_metrics:
                if metric in metrics_text:
                    found_metrics.append(metric)

            print(f"ğŸ“Š Found custom metrics: {found_metrics}")
            return True
        else:
            print(f"âŒ Metrics endpoint returned {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Cannot reach metrics endpoint: {e}")
        return False


def make_analysis_request(tenant: str, scenario: str = "normal"):
    """Make an analysis request."""
    request_data = {
        "request": {
            "period": "2024-01-15T10:30:00Z/2024-01-15T11:30:00Z",
            "tenant": tenant,
            "app": f"{tenant}-app",
            "route": "/api/analysis",
            "required_detectors": ["toxicity", "regex-pii"],
            "observed_coverage": {
                "toxicity": 0.95 if scenario == "normal" else 0.80,
                "regex-pii": 0.93 if scenario == "normal" else 0.75,
            },
            "required_coverage": {"toxicity": 0.95, "regex-pii": 0.95},
            "detector_errors": (
                {} if scenario != "error" else {"toxicity": {"timeout": 10}}
            ),
            "high_sev_hits": (
                []
                if scenario != "incident"
                else [
                    {
                        "detector": "toxicity",
                        "taxonomy": "HARM.SPEECH.Toxicity",
                        "count": 20,
                        "p95_score": 0.98,
                    }
                ]
            ),
            "false_positive_bands": [],
            "policy_bundle": f"{tenant}-policy-1.0",
            "env": "prod",
        }
    }

    headers = {"Content-Type": "application/json", "X-Tenant": tenant}

    try:
        start_time = time.time()
        response = requests.post(
            f"{API_BASE}/api/v1/analysis/analyze",
            json=request_data,
            headers=headers,
            timeout=10,
        )
        latency = (time.time() - start_time) * 1000  # Convert to ms

        if response.status_code == 200:
            result = response.json()
            confidence = result.get("confidence", 0)
            reason = result.get("reason", "")[:50]
            print(
                f"âœ… {tenant}: {latency:.0f}ms, confidence={confidence:.2f}, {reason}..."
            )
            return True, latency
        else:
            print(f"âŒ {tenant}: {response.status_code} - {latency:.0f}ms")
            return False, latency

    except Exception as e:
        print(f"âŒ {tenant}: Error - {str(e)}")
        return False, 0


def simulate_prometheus_queries():
    """Simulate the key Prometheus queries manually."""
    print("\nğŸ“Š SIMULATING GRAFANA DASHBOARD QUERIES")
    print("=" * 60)

    try:
        response = requests.get(f"{API_BASE}/metrics", timeout=5)
        if response.status_code != 200:
            print("âŒ Cannot fetch metrics")
            return

        metrics_text = response.text

        # Parse metrics manually (simplified)
        print("\nğŸ” Key Metrics Found:")

        # Look for request counts
        request_lines = [
            line
            for line in metrics_text.split("\n")
            if "analysis_requests_total" in line and not line.startswith("#")
        ]
        if request_lines:
            print(f"ğŸ“ˆ Request Metrics: {len(request_lines)} series")
            for line in request_lines[:3]:  # Show first 3
                print(f"   {line}")

        # Look for duration metrics
        duration_lines = [
            line
            for line in metrics_text.split("\n")
            if "analysis_request_duration_seconds" in line and not line.startswith("#")
        ]
        if duration_lines:
            print(f"â±ï¸  Latency Metrics: {len(duration_lines)} series")
            for line in duration_lines[:3]:  # Show first 3
                print(f"   {line}")

        # Look for confidence metrics
        confidence_lines = [
            line
            for line in metrics_text.split("\n")
            if "analysis_confidence_score" in line and not line.startswith("#")
        ]
        if confidence_lines:
            print(f"ğŸ¯ Confidence Metrics: {len(confidence_lines)} series")
            for line in confidence_lines[:3]:  # Show first 3
                print(f"   {line}")

    except Exception as e:
        print(f"âŒ Error fetching metrics: {e}")


def demo_slo_monitoring():
    """Demonstrate SLO monitoring in action."""
    print("\nğŸš¨ SLO MONITORING DEMO")
    print("=" * 40)

    print("ğŸ“‹ Golden SLO Targets:")
    print("  â€¢ P95 Latency: < 500ms")
    print("  â€¢ Error Rate: < 1%")
    print("  â€¢ Success Rate: > 99%")

    print("\nğŸª Generating multi-tenant traffic...")

    success_count = 0
    total_count = 0
    latencies = []

    # Generate traffic for each tenant
    for i in range(20):  # 20 requests total
        tenant = random.choice(TENANTS)
        scenario = (
            "normal"
            if random.random() < 0.9
            else random.choice(["coverage_gap", "incident"])
        )

        success, latency = make_analysis_request(tenant, scenario)
        total_count += 1
        if success:
            success_count += 1
        if latency > 0:
            latencies.append(latency)

        time.sleep(0.5)  # Brief pause between requests

    # Calculate SLO metrics
    if latencies:
        latencies.sort()
        p95_index = int(0.95 * len(latencies))
        p95_latency = (
            latencies[p95_index] if p95_index < len(latencies) else latencies[-1]
        )
        avg_latency = sum(latencies) / len(latencies)
    else:
        p95_latency = 0
        avg_latency = 0

    success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
    error_rate = (
        ((total_count - success_count) / total_count) * 100 if total_count > 0 else 0
    )

    print(f"\nğŸ“Š SLO RESULTS:")
    print(
        f"  ğŸ“ˆ Success Rate: {success_rate:.1f}% ({'âœ…' if success_rate > 99 else 'âŒ'} SLO)"
    )
    print(
        f"  ğŸ“‰ Error Rate: {error_rate:.1f}% ({'âœ…' if error_rate < 1 else 'âŒ'} SLO)"
    )
    print(f"  â±ï¸  Average Latency: {avg_latency:.0f}ms")
    print(
        f"  ğŸš€ P95 Latency: {p95_latency:.0f}ms ({'âœ…' if p95_latency < 500 else 'âŒ'} SLO)"
    )

    # Simulate alerts
    print(f"\nğŸš¨ ALERT STATUS:")
    if p95_latency > 500:
        print("  ğŸ”´ CRITICAL: P95 latency exceeds 500ms SLO")
    if error_rate > 1:
        print("  ğŸ”´ CRITICAL: Error rate exceeds 1% SLO")
    if success_rate < 99:
        print("  ğŸŸ¡ WARNING: Success rate below 99% SLO")

    if p95_latency <= 500 and error_rate <= 1 and success_rate >= 99:
        print("  âœ… All SLOs within targets")


def main():
    """Run the standalone monitoring demo."""
    print("ğŸª STANDALONE MONITORING DEMO")
    print("=" * 50)
    print("Demonstrates enterprise-grade monitoring without Docker")
    print()

    # Step 1: Check metrics endpoint
    if not check_metrics_endpoint():
        print("\nâŒ Metrics endpoint not available. Make sure analysis API is running.")
        return

    # Step 2: Show current metrics
    simulate_prometheus_queries()

    # Step 3: Generate traffic and show SLO monitoring
    demo_slo_monitoring()

    # Step 4: Show final metrics
    print("\nğŸ”„ Final metrics check...")
    simulate_prometheus_queries()

    print("\nğŸ¯ DEMO COMPLETE!")
    print("\nğŸ’¡ Key Takeaways:")
    print("  âœ… Metrics endpoint exposes Prometheus-compatible metrics")
    print("  âœ… Multi-tenant request tracking with per-tenant SLAs")
    print("  âœ… Real-time SLO monitoring (latency, error rate, success rate)")
    print("  âœ… Golden alerts for P95 > 500ms and error rate > 1%")
    print("  âœ… Enterprise-ready observability stack")

    print(f"\nğŸ“Š Access live metrics: {API_BASE}/metrics")
    print("ğŸš€ Ready for Grafana integration when Docker is available!")


if __name__ == "__main__":
    main()
