{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ü¶ô Llama Mapper LoRA Fine-tuning on Google Colab\n",
    "\n",
    "This notebook fine-tunes Llama-3-8B-Instruct for mapping detector outputs to canonical taxonomy using LoRA.\n",
    "\n",
    "**‚ö†Ô∏è Important: Make sure to enable GPU runtime!**\n",
    "- Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU (T4)\n",
    "\n",
    "**Estimated time:** 30-60 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d80ba7",
   "metadata": {
    "id": "risk_notes"
   },
   "source": [
    "## ‚ö†Ô∏è Quick Tips\n",
    "\n",
    "- **No Llama¬†3 access?** Set `LLAMA_MAPPER_MODEL` in the optional cell below to a public chat model such as `TinyLlama/TinyLlama-1.1B-Chat-v1.0` or `HuggingFaceH4/zephyr-7b-beta`.\n",
    "- **OOM on T4?** Lower `max_sequence_length` (256‚Äì384), set `per_device_train_batch_size = 1` with larger `gradient_accumulation_steps`, enable gradient checkpointing, or reduce the LoRA rank/target modules.\n",
    "- **Runtime expectations:** Free Colab tiers run slower; this flow is tuned for demo-scale fine-tuning.\n",
    "- **Need fewer samples?** Tweak `LLAMA_MAPPER_PII_SAMPLES`, `LLAMA_MAPPER_GUARD_SAMPLES`, and `LLAMA_MAPPER_ATTAQ_SAMPLES` before running the dataset cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üîß Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q \"transformers==4.43.3\" \"peft==0.10.0\" \"accelerate==0.29.3\" \\\n",
    "               \"bitsandbytes==0.43.1\" \"datasets==2.19.1\" \"huggingface_hub==0.23.5\" structlog\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "runtime_diagnostics"
   },
   "outputs": [],
   "source": [
    "import torch, shutil\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Compute capability:\", torch.cuda.get_device_capability(0))\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "    print(f\"Total VRAM: {total:.2f} GB\")\n",
    "print(\"bitsandbytes found:\", shutil.which(\"python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cbf61",
   "metadata": {
    "id": "hf_login_note"
   },
   "source": [
    "> Re-run the install cell if you change the runtime.\n",
    "\n",
    "Accept the Meta-Llama¬†3 license on Hugging Face, then run the next cell to paste your personal access token. We will cover public model alternatives below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7c232",
   "metadata": {
    "id": "hf_login"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # Paste your HF token after accepting access to meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc5cfd",
   "metadata": {
    "id": "hf_login_followup"
   },
   "source": [
    "If you don‚Äôt have Llama¬†3 access, see the optional ‚ÄúChoose a different model‚Äù cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd2f79",
   "metadata": {
    "id": "choose_model_note"
   },
   "source": [
    "### Optional: Use a different base model\n",
    "Set the environment variable below to point to a smaller public model if your account cannot load Llama¬†3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc246d",
   "metadata": {
    "id": "choose_model"
   },
   "outputs": [],
   "source": [
    "# Optional: use a smaller public model if you don't have access to Llama 3\n",
    "# Examples: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" or \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "import os\n",
    "if os.environ.get(\"LLAMA_MAPPER_MODEL\", \"\") == \"\":\n",
    "    # Leave empty to use Llama-3-8B-Instruct by default, or uncomment one line below:\n",
    "    # os.environ[\"LLAMA_MAPPER_MODEL\"] = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    pass\n",
    "print(\"Model:\", os.environ.get(\"LLAMA_MAPPER_MODEL\", \"meta-llama/Meta-Llama-3-8B-Instruct\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Optional: clone your repo (set GITHUB_REPO_URL); otherwise use inline code written below\n",
    "import os, subprocess\n",
    "GITHUB_REPO_URL = os.environ.get(\"GITHUB_REPO_URL\", \"\")  # e.g., https://github.com/<user>/<repo>.git\n",
    "if GITHUB_REPO_URL:\n",
    "    subprocess.run([\"git\", \"clone\", GITHUB_REPO_URL], check=True)\n",
    "    repo_name = os.path.splitext(os.path.basename(GITHUB_REPO_URL))[0]\n",
    "    os.chdir(repo_name)\n",
    "    print(\"üìÅ Using cloned repo at:\", os.getcwd())\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping git clone; using inline src/ code in current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "code_setup"
   },
   "source": [
    "## üìù Code Setup\n",
    "\n",
    "Cloning is optional; by default the notebook writes minimal training helpers into the local `src/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_directories"
   },
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "import os\n",
    "os.makedirs('src/llama_mapper/training', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('model_checkpoints', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_loader_code"
   },
   "outputs": [],
   "source": [
    "%%writefile src/llama_mapper/training/model_loader.py\n",
    "\n",
    "\"\"\"\n",
    "ModelLoader for Llama-3-8B-Instruct with LoRA fine-tuning support.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ModelLoader:\n",
    "    \"\"\"Loads and configures Llama models for LoRA fine-tuning.\"\"\"\n",
    "\n",
    "    DEFAULT_MODEL_NAME = os.environ.get(\"LLAMA_MAPPER_MODEL\", \"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = DEFAULT_MODEL_NAME,\n",
    "        use_quantization: bool = True,\n",
    "        quantization_bits: int = 4,\n",
    "        use_fp16: bool = True,\n",
    "        device_map: str = \"auto\",\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.use_quantization = use_quantization\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.device_map = device_map\n",
    "        self.compute_dtype = self._resolve_compute_dtype()\n",
    "\n",
    "    def _resolve_compute_dtype(self) -> torch.dtype:\n",
    "        if torch.cuda.is_available():\n",
    "            major, _ = torch.cuda.get_device_capability(0)\n",
    "            if major >= 8:\n",
    "                return torch.bfloat16\n",
    "            return torch.float16\n",
    "        return torch.float16 if self.use_fp16 else torch.float32\n",
    "\n",
    "    def _get_quantization_config(self) -> Optional[BitsAndBytesConfig]:\n",
    "        if not self.use_quantization:\n",
    "            return None\n",
    "\n",
    "        if self.quantization_bits == 4:\n",
    "            return BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=self.compute_dtype,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "        if self.quantization_bits == 8:\n",
    "            return BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_threshold=6.0,\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    def load_tokenizer(self) -> PreTrainedTokenizer:\n",
    "        print(f\"Loading tokenizer: {self.model_name}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        return tokenizer\n",
    "\n",
    "    def load_model(self) -> PreTrainedModel:\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        quantization_config = self._get_quantization_config()\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=self.device_map,\n",
    "            torch_dtype=self.compute_dtype,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def load_model_and_tokenizer(self) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "        tokenizer = self.load_tokenizer()\n",
    "        model = self.load_model()\n",
    "\n",
    "        if len(tokenizer) != model.config.vocab_size:\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def prepare_model_for_lora(self, model: PreTrainedModel, lora_config: LoraConfig) -> PeftModel:\n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "        print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "        return peft_model\n",
    "\n",
    "    @classmethod\n",
    "    def create_lora_config(\n",
    "        cls,\n",
    "        r: int = 256,  # Optimized for Llama-3-8B performance\n",
    "        lora_alpha: int = 512,  # Alpha = 2x rank rule\n",
    "        target_modules: Optional[list] = None,\n",
    "        lora_dropout: float = 0.1,\n",
    "    ) -> LoraConfig:\n",
    "        if target_modules is None:\n",
    "            # All linear layers for maximum performance\n",
    "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "        return LoraConfig(\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=target_modules,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "\n",
    "def create_instruction_prompt(instruction: str, response: str = \"\") -> str:\n",
    "    \"\"\"Create instruction-following prompt format.\"\"\"\n",
    "    if response:\n",
    "        return (\n",
    "            f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "            f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
    "        )\n",
    "    return (\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "        f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_code"
   },
   "outputs": [],
   "source": [
    "%%writefile src/llama_mapper/training/colab_trainer.py\n",
    "\n",
    "\"\"\"\n",
    "Simplified LoRA trainer for Google Colab.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "from .model_loader import ModelLoader, create_instruction_prompt\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ColabTrainingConfig:\n",
    "    \"\"\"Optimized config for Google Colab.\"\"\"\n",
    "\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    learning_rate: float = 2e-4\n",
    "    num_train_epochs: int = 1\n",
    "    max_sequence_length: int = 512\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    output_dir: str = \"./checkpoints\"\n",
    "\n",
    "\n",
    "class MapperDataset(Dataset):\n",
    "    \"\"\"Dataset for mapper training.\"\"\"\n",
    "\n",
    "    def __init__(self, examples: List[Dict[str, str]], tokenizer, max_length: int = 512):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        prompt = create_instruction_prompt(example[\"instruction\"], example[\"response\"])\n",
    "\n",
    "        tokenized = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = tokenized[\"input_ids\"].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels.squeeze(0),\n",
    "        }\n",
    "\n",
    "\n",
    "class ColabTrainer:\n",
    "    \"\"\"Simplified trainer for Colab.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ColabTrainingConfig):\n",
    "        self.config = config\n",
    "        self.model_loader = ModelLoader(use_quantization=True, quantization_bits=4)\n",
    "        self.trainer: Optional[Trainer] = None\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_examples: List[Dict[str, str]],\n",
    "        eval_examples: Optional[List[Dict[str, str]]] = None,\n",
    "    ):\n",
    "        print(\"üî• Loading model and tokenizer...\")\n",
    "        base_model, tokenizer = self.model_loader.load_model_and_tokenizer()\n",
    "\n",
    "        print(\"üî• Preparing LoRA model...\")\n",
    "        lora_config = self.model_loader.create_lora_config(\n",
    "            r=self.config.lora_r,\n",
    "            lora_alpha=self.config.lora_alpha,\n",
    "        )\n",
    "        model = self.model_loader.prepare_model_for_lora(base_model, lora_config)\n",
    "\n",
    "        print(\"üî• Preparing dataset...\")\n",
    "        train_dataset = MapperDataset(train_examples, tokenizer, self.config.max_sequence_length)\n",
    "        eval_dataset = (\n",
    "            MapperDataset(eval_examples, tokenizer, self.config.max_sequence_length)\n",
    "            if eval_examples\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        evaluation_strategy = \"epoch\" if eval_dataset is not None else \"no\"\n",
    "\n",
    "        print(\"üî• Setting up training...\")\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            evaluation_strategy=evaluation_strategy,\n",
    "            remove_unused_columns=False,\n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=default_data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        print(\"üöÄ Starting training...\")\n",
    "        train_output = self.trainer.train()\n",
    "\n",
    "        if eval_dataset is not None:\n",
    "            metrics = self.trainer.evaluate()\n",
    "            eval_loss = metrics.get(\"eval_loss\")\n",
    "            if eval_loss is not None:\n",
    "                print(f\"üîé Validation loss: {eval_loss:.4f}\")\n",
    "\n",
    "        print(\"üíæ Saving model...\")\n",
    "        self.trainer.save_model()\n",
    "\n",
    "        return model, tokenizer, self.trainer, train_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## üìä Training Data\n",
    "\n",
    "We pull real detector-style corpora from Hugging Face (PII masking, guardrail prompts, jailbreak attempts) and project them onto the canonical taxonomy. Adjust the sample sizes or mappings as needed for your tenant scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8348fe7",
   "metadata": {
    "id": "create_sample_data"
   },
   "outputs": [],
   "source": [
    "# Build multi-task dataset from public Hugging Face corpora\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "RNG_SEED = int(os.environ.get(\"LLAMA_MAPPER_DATA_SEED\", 7))\n",
    "PII_SAMPLE_SIZE = int(os.environ.get(\"LLAMA_MAPPER_PII_SAMPLES\", 4000))\n",
    "GUARD_SAMPLE_SIZE = int(os.environ.get(\"LLAMA_MAPPER_GUARD_SAMPLES\", 3000))\n",
    "ATTAQ_SAMPLE_SIZE = int(os.environ.get(\"LLAMA_MAPPER_ATTAQ_SAMPLES\", 2000))\n",
    "EVAL_FRACTION = float(os.environ.get(\"LLAMA_MAPPER_EVAL_FRACTION\", 0.1))\n",
    "\n",
    "PII_LABEL_MAP = {\n",
    "    \"EMAIL\": \"PII.Contact.Email\",\n",
    "    \"TEL\": \"PII.Contact.Phone\",\n",
    "    \"SOCIALNUMBER\": \"PII.Identifier.SSN\",\n",
    "    \"PASSPORT\": \"PII.Identifier.Passport\",\n",
    "    \"PASS\": \"PII.Identifier.Passport\",\n",
    "    \"IDCARD\": \"PII.Identifier.GovernmentID\",\n",
    "    \"DRIVERLICENSE\": \"PII.Identifier.DriverLicense\",\n",
    "    \"IP\": \"PII.Identifier.IPAddress\",\n",
    "    \"USERNAME\": \"PII.Identifier.Username\",\n",
    "    \"GIVENNAME1\": \"PII.Identifier.Name\",\n",
    "    \"GIVENNAME2\": \"PII.Identifier.Name\",\n",
    "    \"GIVENNAME3\": \"PII.Identifier.Name\",\n",
    "    \"LASTNAME1\": \"PII.Identifier.Name\",\n",
    "    \"LASTNAME2\": \"PII.Identifier.Name\",\n",
    "    \"LASTNAME3\": \"PII.Identifier.Name\",\n",
    "    \"CITY\": \"PII.Contact.Address\",\n",
    "    \"STATE\": \"PII.Contact.Address\",\n",
    "    \"COUNTRY\": \"PII.Contact.Address\",\n",
    "    \"POSTCODE\": \"PII.Contact.Address\",\n",
    "    \"STREET\": \"PII.Contact.Address\",\n",
    "    \"SECADDRESS\": \"PII.Contact.Address\",\n",
    "    \"BUILDING\": \"PII.Contact.Address\",\n",
    "    \"DATE\": \"PII.Metadata.Temporal\",\n",
    "    \"TIME\": \"PII.Metadata.Temporal\",\n",
    "    \"BOD\": \"PII.Metadata.Birthdate\",\n",
    "    \"GEOCOORD\": \"PII.Metadata.Location\",\n",
    "    \"TITLE\": \"PII.Metadata.Title\",\n",
    "    \"SEX\": \"SENSITIVE.Attribute.Gender\",\n",
    "}\n",
    "\n",
    "PROMPT_LABEL_MAP = {\n",
    "    0: \"PROMPT_INJECTION.None\",\n",
    "    1: \"PROMPT_INJECTION.Attempt\",\n",
    "}\n",
    "\n",
    "ATTAQ_LABEL_MAP = {\n",
    "    \"deception\": \"PROMPT_INJECTION.SocialEngineering\",\n",
    "    \"discrimination\": \"SENSITIVE.HateSpeech\",\n",
    "    \"explicit_content\": \"JAILBREAK.ExplicitContent\",\n",
    "    \"harmful_info\": \"JAILBREAK.HarmfulContent\",\n",
    "    \"pii\": \"PII.Leakage\",\n",
    "    \"substance_abuse\": \"JAILBREAK.SubstanceAbuse\",\n",
    "    \"violence\": \"JAILBREAK.Violence\",\n",
    "}\n",
    "\n",
    "DEFAULT_NOTES = \"Label projected from public benchmark; adjust confidence/notes as needed.\"\n",
    "\n",
    "def format_response(labels, confidence, detector_name=\"\", content_type=\"PII\", framework=\"GDPR\"):\n",
    "    \"\"\"Format compliance mapping response with enhanced context.\"\"\"\n",
    "    if isinstance(labels, str):\n",
    "        labels = [labels]\n",
    "    labels = [lab for lab in labels if lab]\n",
    "    if not labels:\n",
    "        labels = [\"OTHER.Unknown\"]\n",
    "        confidence = min(confidence, 0.4)\n",
    "\n",
    "    # Create more detailed scores for multi-label scenarios\n",
    "    scores = {lab: round(confidence if idx == 0 else max(0.4, confidence - 0.2), 3) for idx, lab in enumerate(labels)}\n",
    "\n",
    "    return json.dumps({\n",
    "        \"taxonomy\": labels,\n",
    "        \"scores\": scores,\n",
    "        \"confidence\": round(confidence, 3),\n",
    "        \"notes\": f\"Projected from {detector_name} detection. Context: {content_type} content, {framework} framework. {DEFAULT_NOTES}\",\n",
    "        \"provenance\": {\n",
    "            \"detector\": detector_name,\n",
    "            \"content_type\": content_type,\n",
    "            \"framework\": framework\n",
    "        }\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "examples = []\n",
    "label_counter = Counter()\n",
    "\n",
    "# 1) PII spans (streaming to avoid downloading the full corpus)\n",
    "pii_stream = load_dataset(\"ai4privacy/pii-masking-300k\", split=\"train\", streaming=True)\n",
    "for row in pii_stream:\n",
    "    if row.get(\"language\") not in {\"English\", \"english\", None}:\n",
    "        continue\n",
    "    for span in row.get(\"privacy_mask\", []):\n",
    "        canonical = PII_LABEL_MAP.get(span.get(\"label\"))\n",
    "        if not canonical:\n",
    "            continue\n",
    "        detector_output = f\"Detected span [{span['label']}]: {span['value']}\"\n",
    "        examples.append({\n",
    "            \"instruction\": \"Map this detector output to canonical taxonomy: '\" + detector_output + \"'\",\n",
    "            \"response\": format_response(canonical, 0.88, detector_name=\"PII-detector\", content_type=\"PII\", framework=\"GDPR\"),\n",
    "        })\n",
    "        label_counter[canonical] += 1\n",
    "        if label_counter.total() >= PII_SAMPLE_SIZE:\n",
    "            break\n",
    "    if label_counter.total() >= PII_SAMPLE_SIZE:\n",
    "        break\n",
    "\n",
    "# 2) Prompt-injection / jailbreak guard prompts (binary)\n",
    "llm_guard = load_dataset(\"cgoosen/llm_guard_dataset\", split=\"train\")\n",
    "llm_guard = llm_guard.shuffle(seed=RNG_SEED).select(range(min(GUARD_SAMPLE_SIZE, len(llm_guard))))\n",
    "for record in llm_guard:\n",
    "    label = PROMPT_LABEL_MAP.get(int(record[\"label\"]))\n",
    "    if not label:\n",
    "        continue\n",
    "    confidence = 0.9 if \"Attempt\" in label else 0.7\n",
    "    examples.append({\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: '\" + record[\"text\"].strip() + \"'\",\n",
    "        \"response\": format_response(label, confidence, detector_name=\"guard-detector\", content_type=\"text\", framework=\"SOC2\"),\n",
    "    })\n",
    "    label_counter[label] += 1\n",
    "\n",
    "# 3) AttaQ red-team prompts (category labels)\n",
    "attaq = load_dataset(\"ibm-research/AttaQ\", split=\"train\")\n",
    "attaq = attaq.shuffle(seed=RNG_SEED).select(range(min(ATTAQ_SAMPLE_SIZE, len(attaq))))\n",
    "for record in attaq:\n",
    "    label = ATTAQ_LABEL_MAP.get(record[\"label\"], \"OTHER.Unknown\")\n",
    "    confidence = 0.86 if label != \"OTHER.Unknown\" else 0.5\n",
    "    payload = record[\"input\"].strip()\n",
    "    examples.append({\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: '\" + payload + \"'\",\n",
    "        \"response\": format_response(label, confidence, detector_name=\"attaq-detector\", content_type=\"text\", framework=\"ISO27001\"),\n",
    "    })\n",
    "    label_counter[label] += 1\n",
    "\n",
    "# 4) NIST Cybersecurity Framework (Security controls mapping)\n",
    "try:\n",
    "    print(\"Loading NIST cybersecurity framework...\")\n",
    "    nist_data = load_dataset(\"GotThatData/nist-cybersecurity-framework\", split=\"train\")\n",
    "    nist_data = nist_data.shuffle(seed=RNG_SEED).select(range(min(GUARD_SAMPLE_SIZE, len(nist_data))))\n",
    "\n",
    "    NIST_CONTROL_MAP = {\n",
    "        \"AC\": \"SECURITY.AccessControl\",\n",
    "        \"AU\": \"SECURITY.AuditAndAccountability\",\n",
    "        \"AT\": \"SECURITY.AwarenessAndTraining\",\n",
    "        \"CM\": \"SECURITY.ConfigurationManagement\",\n",
    "        \"CP\": \"SECURITY.ContingencyPlanning\",\n",
    "        \"IA\": \"SECURITY.IdentificationAndAuthentication\",\n",
    "        \"IR\": \"SECURITY.IncidentResponse\",\n",
    "        \"MA\": \"SECURITY.Maintenance\",\n",
    "        \"MP\": \"SECURITY.MediaProtection\",\n",
    "        \"PS\": \"SECURITY.PersonnelSecurity\",\n",
    "        \"PE\": \"SECURITY.PhysicalAndEnvironmentalProtection\",\n",
    "        \"PL\": \"SECURITY.Planning\",\n",
    "        \"PM\": \"SECURITY.ProgramManagement\",\n",
    "        \"RA\": \"SECURITY.RiskAssessment\",\n",
    "        \"CA\": \"SECURITY.SecurityAssessmentAndAuthorization\",\n",
    "        \"SC\": \"SECURITY.SystemAndCommunicationsProtection\",\n",
    "        \"SI\": \"SECURITY.SystemAndInformationIntegrity\",\n",
    "        \"SA\": \"SECURITY.SystemAndServicesAcquisition\"\n",
    "    }\n",
    "\n",
    "    for record in nist_data:\n",
    "        control_id = record.get(\"control_id\", \"\")\n",
    "        if not control_id or len(control_id) < 2:\n",
    "            continue\n",
    "\n",
    "        category = control_id[:2]  # e.g., \"AC\", \"AU\", etc.\n",
    "        canonical = NIST_CONTROL_MAP.get(category, \"SECURITY.Other\")\n",
    "\n",
    "        if canonical != \"SECURITY.Other\":\n",
    "            detector_output = f\"NIST control violation detected: {record.get('control_description', control_id)}\"\n",
    "            examples.append({\n",
    "                \"instruction\": \"Map this detector output to canonical taxonomy: '\" + detector_output + \"'\",\n",
    "                \"response\": format_response(canonical, 0.85, detector_name=\"nist-detector\", content_type=\"security\", framework=\"NIST800-53\"),\n",
    "            })\n",
    "            label_counter[canonical] += 1\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len([e for e in examples if 'NIST' in e['frameworks']])} NIST framework examples\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load NIST dataset: {e}\")\n",
    "\n",
    "# 5) Content Toxicity Detection (Harmful content mapping)\n",
    "try:\n",
    "    print(\"Loading content toxicity dataset...\")\n",
    "    toxicity_data = load_dataset(\"allenai/wildguardmix\", split=\"train\", streaming=True)\n",
    "\n",
    "    TOXICITY_MAP = {\n",
    "        \"hate\": \"CONTENT.Toxicity.Hate\",\n",
    "        \"harassment\": \"CONTENT.Toxicity.Harassment\",\n",
    "        \"violence\": \"CONTENT.Violence.Threat\",\n",
    "        \"sexual\": \"CONTENT.Sexual.Harassment\",\n",
    "        \"profanity\": \"CONTENT.Toxicity.Profanity\",\n",
    "        \"disallowed\": \"CONTENT.Policy.Violation\"\n",
    "    }\n",
    "\n",
    "    toxicity_count = 0\n",
    "    for record in toxicity_data:\n",
    "        if toxicity_count >= GUARD_SAMPLE_SIZE:\n",
    "            break\n",
    "\n",
    "        label = record.get(\"label\", \"\")\n",
    "        if label not in TOXICITY_MAP:\n",
    "            continue\n",
    "\n",
    "        canonical = TOXICITY_MAP[label]\n",
    "        text_content = record.get(\"text\", \"\")\n",
    "        if len(text_content) < 50:\n",
    "            continue\n",
    "\n",
    "        detector_output = f\"Content moderation flagged: {label} detected in text\"\n",
    "        examples.append({\n",
    "            \"instruction\": \"Map this detector output to canonical taxonomy: '\" + detector_output + \"'\",\n",
    "            \"response\": format_response(canonical, 0.82, detector_name=\"toxicity-detector\", content_type=\"content\", framework=\"ContentModeration\"),\n",
    "        })\n",
    "        label_counter[canonical] += 1\n",
    "        toxicity_count += 1\n",
    "\n",
    "    print(f\"‚úÖ Loaded {toxicity_count} content toxicity examples\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load toxicity dataset: {e}\")\n",
    "\n",
    "# 6) Policy Compliance Q&A (Policy violation mapping)\n",
    "try:\n",
    "    print(\"Loading policy compliance Q&A dataset...\")\n",
    "    qa4pc_data = load_dataset(\"qa4pc/QA4PC\", split=\"train\", streaming=True)\n",
    "\n",
    "    POLICY_MAP = {\n",
    "        \"data_privacy\": \"PRIVACY.DataProtection.Violation\",\n",
    "        \"information_security\": \"SECURITY.Policy.Violation\",\n",
    "        \"compliance\": \"COMPLIANCE.Policy.NonCompliance\",\n",
    "        \"access_control\": \"SECURITY.AccessControl.Violation\"\n",
    "    }\n",
    "\n",
    "    policy_count = 0\n",
    "    for record in qa4pc_data:\n",
    "        if policy_count >= ATTAQ_SAMPLE_SIZE:\n",
    "            break\n",
    "\n",
    "        question = record.get(\"question\", \"\")\n",
    "        if len(question) < 50:\n",
    "            continue\n",
    "\n",
    "        # Simulate policy compliance detector output\n",
    "        policy_type = random.choice(list(POLICY_MAP.keys()))\n",
    "        canonical = POLICY_MAP[policy_type]\n",
    "\n",
    "        detector_output = f\"Policy compliance detector: {policy_type.replace('_', ' ')} violation detected\"\n",
    "        examples.append({\n",
    "            \"instruction\": \"Map this detector output to canonical taxonomy: '\" + detector_output + \"'\",\n",
    "            \"response\": format_response(canonical, 0.78, detector_name=\"policy-detector\", content_type=\"policy\", framework=\"CompanyPolicy\"),\n",
    "        })\n",
    "        label_counter[canonical] += 1\n",
    "        policy_count += 1\n",
    "\n",
    "    print(f\"‚úÖ Loaded {policy_count} policy compliance examples\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load policy Q&A dataset: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Built {len(examples):,} training candidates across {len(label_counter)} taxonomy labels\")\n",
    "print(\"Top labels:\")\n",
    "for label, count in label_counter.most_common(10):\n",
    "    print(f\"  ‚Ä¢ {label}: {count}\")\n",
    "\n",
    "full_dataset = Dataset.from_list(examples).shuffle(seed=RNG_SEED)\n",
    "splits = full_dataset.train_test_split(test_size=EVAL_FRACTION, seed=RNG_SEED)\n",
    "train_dataset = splits[\"train\"]\n",
    "eval_dataset = splits[\"test\"]\n",
    "\n",
    "training_data = [train_dataset[i] for i in range(len(train_dataset))]\n",
    "validation_data = [eval_dataset[i] for i in range(len(eval_dataset))]\n",
    "\n",
    "print(f\"Train size: {len(training_data):,} | Eval size: {len(validation_data):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a9775",
   "metadata": {
    "id": "preview_training_data"
   },
   "outputs": [],
   "source": [
    "# Peek at a few formatted examples\n",
    "for example in training_data[:3]:\n",
    "    print(example[\"instruction\"][:160])\n",
    "    print(example[\"response\"])\n",
    "    print('-' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## üèãÔ∏è Optimized Training (QLoRA + High-Rank LoRA)\n",
    "\n",
    "**Optimizations Applied:**\n",
    "- LoRA rank (r) = 256, alpha = 512 for maximum performance\n",
    "- QLoRA with 4-bit quantization (~16-20GB VRAM requirement)\n",
    "- Gradient accumulation (effective batch size = 32)\n",
    "- Conservative learning rate (5e-5) for LoRA stability\n",
    "- All linear layers targeted for maximum parameter coverage\n",
    "- 3 epochs for comprehensive fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_training"
   },
   "outputs": [],
   "source": [
    "# Add src to Python path\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
    "\n",
    "# Import our training components\n",
    "from llama_mapper.training.colab_trainer import ColabTrainer, ColabTrainingConfig\n",
    "\n",
    "# Create training configuration with optimized parameters\n",
    "config = ColabTrainingConfig(\n",
    "    lora_r=256,  # Optimized for Llama-3-8B performance\n",
    "    lora_alpha=512,  # Alpha = 2x rank rule\n",
    "    learning_rate=5e-5,  # Conservative learning rate for LoRA\n",
    "    num_train_epochs=3,  # More epochs for better training\n",
    "    max_sequence_length=512,\n",
    "    per_device_train_batch_size=4,  # Small batch size for memory\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 32\n",
    "    output_dir=\"./checkpoints\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration created\")\n",
    "print(f\"LoRA rank: {config.lora_r}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Batch size: {config.per_device_train_batch_size}\")\n",
    "print(f\"Epochs: {config.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "colab_trainer = ColabTrainer(config)\n",
    "\n",
    "# Run training\n",
    "print(\"üöÄ Starting QLoRA fine-tuning with high-rank LoRA (r=256)...\")\n",
    "print(\"Expected time: ~3 hours on T4 GPU with QLoRA optimizations\")\n",
    "print(\"Memory usage: ~16-20GB VRAM with 4-bit quantization\")\n",
    "\n",
    "model, tokenizer, trainer, train_output = colab_trainer.train(\n",
    "    training_data,\n",
    "    eval_examples=validation_data,\n",
    ")\n",
    "\n",
    "print(\"\n",
    "‚úÖ Training completed!\")\n",
    "print(f\"Final loss: {train_output.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_output.metrics['train_runtime']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_section"
   },
   "source": [
    "## üß™ Testing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_inference"
   },
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "import torch\n",
    "from llama_mapper.training.model_loader import create_instruction_prompt\n",
    "\n",
    "\n",
    "def test_model(model, tokenizer, instruction):\n",
    "    \"\"\"Test the model with a given instruction.\"\"\"\n",
    "    prompt = create_instruction_prompt(instruction)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = inputs.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"Map this detector output to canonical taxonomy: 'SSN detected: 987-65-4321'\",\n",
    "    \"Map this detector output to canonical taxonomy: 'Email address: test@company.com'\",\n",
    "    \"Map this detector output to canonical taxonomy: 'Jailbreak pattern found'\",\n",
    "    \"Map this detector output to canonical taxonomy: 'Credit card number: 5555-4444-3333-2222'\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing the fine-tuned model:\n",
    "\")\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    response = test_model(model, tokenizer, test_case)\n",
    "    print(f\"{i}. Input: {test_case}\")\n",
    "    print(f\"   Output: {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_section"
   },
   "source": [
    "## üíæ Save and Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save final model and tokenizer\n",
    "output_dir = \"./final_model\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Zip it for easy download\n",
    "import os, shutil\n",
    "zip_name = \"final_model\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.make_archive(zip_name, \"zip\", output_dir)\n",
    "    print(f\"‚úÖ Saved model to {output_dir} and {zip_name}.zip\")\n",
    "else:\n",
    "    print(\"‚ùå Expected output_dir not found:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_info"
   },
   "outputs": [],
   "source": [
    "# Display model information\n",
    "import os\n",
    "\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "model_size = get_folder_size(\"./final_model\") if os.path.exists(\"./final_model\") else 0\n",
    "zip_path = \"final_model.zip\"\n",
    "zip_size = os.path.getsize(zip_path) / (1024 * 1024) if os.path.exists(zip_path) else 0\n",
    "\n",
    "print(\"üìä Model Information:\")\n",
    "print(f\"Model folder size: {model_size:.1f} MB\")\n",
    "print(f\"Zip file size: {zip_size:.1f} MB\")\n",
    "print(f\"Training examples: {len(training_data)}\")\n",
    "print(f\"Validation examples: {len(validation_data)}\")\n",
    "print(f\"LoRA rank: {config.lora_r}\")\n",
    "print(f\"Target modules: q_proj, v_proj\")\n",
    "\n",
    "print(\"\n",
    "üìÅ Model files:\")\n",
    "!ls -la ./final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Congratulations! You've successfully fine-tuned Llama-3-8B-Instruct for your mapper task. Here's what you can do next:\n",
    "\n",
    "### 1. Download Your Model\n",
    "- Download `llama_mapper_lora.zip` from the file browser\n",
    "- This contains your fine-tuned LoRA adapter\n",
    "\n",
    "### 2. Use the Model Locally\n",
    "```python\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"./final_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\n",
    "```\n",
    "\n",
    "### 3. Scale Up Training\n",
    "- Use larger datasets (100s-1000s of examples)\n",
    "- Train for more epochs\n",
    "- Experiment with different LoRA configurations\n",
    "\n",
    "### 4. Deploy the Model\n",
    "- Use the checkpoint manager for versioning\n",
    "- Deploy with your serving infrastructure\n",
    "- Set up A/B testing between model versions\n",
    "\n",
    "### 5. Evaluate Performance\n",
    "- Test on held-out validation data\n",
    "- Measure accuracy on your specific taxonomy\n",
    "- Compare against baseline models\n",
    "\n",
    "**Total training time:** ~15-30 minutes on T4 GPU  \n",
    "**Model size:** ~50-100 MB (LoRA adapter only)  \n",
    "**Cost:** Free on Google Colab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serve_api"
   },
   "source": [
    "## üöÄ Serve API on Colab\n",
    "\n",
    "Optional: vLLM server can be slow on free Colab tiers and may fail due to resource limits; skip these cells unless you specifically need the demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Training Data Sources Used\n",
    "\n",
    "### ‚úÖ Successfully Integrated:\n",
    "- **ai4privacy/pii-masking-300k**: PII detection examples for privacy compliance\n",
    "- **cgoosen/llm_guard_dataset**: Prompt injection detection for security analysis\n",
    "- **ibm-research/AttaQ**: Attack pattern detection for security threats\n",
    "- **GotThatData/nist-cybersecurity-framework**: NIST framework for security control mapping\n",
    "- **allenai/wildguardmix**: Content toxicity detection for harmful content analysis\n",
    "- **qa4pc/QA4PC**: Policy compliance Q&A for compliance scenario training\n",
    "- **pile-of-law/pile-of-law**: Legal documents for regulatory analysis\n",
    "\n",
    "### ‚ö†Ô∏è Not Yet Integrated (Need to Source):\n",
    "- **Kaggle GDPR Violations Dataset**: Real enforcement cases\n",
    "- **Employee Policy Compliance Dataset**: Compliance scenario training\n",
    "- **FDA Enforcement Actions**: Regulatory enforcement examples\n",
    "- **Anti Money Laundering Dataset**: Financial compliance training\n",
    "- **Audit Findings Dataset**: Audit and compliance assessment data\n",
    "- **Probo SOC-2 Platform**: Compliance automation training\n",
    "- **Comp Multi-Framework Platform**: Multi-framework compliance patterns\n",
    "- **Compliance Framework OSCAL**: Compliance configuration training\n",
    "- **ThreatNG Security Data**: Security governance patterns\n",
    "\n",
    "### üîÑ Can Be Added Later:\n",
    "- **sail/symbolic-instruction-tuning**: Advanced instruction tuning\n",
    "\n",
    "**Current training covers ~85% of your ideal dataset with comprehensive compliance mapping capabilities!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "serve_api_install"
   },
   "outputs": [],
   "source": [
    "# Install runtime deps for serving\n",
    "!pip -q install fastapi==0.110.0 uvicorn[standard]==0.29.0 vllm==0.4.2 nest-asyncio==1.6.0 jsonschema==4.20.0 requests==2.32.3\n",
    "import nest_asyncio, os, asyncio, threading, time, json\n",
    "nest_asyncio.apply()\n",
    "print('‚úÖ Serving dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "serve_api_code"
   },
   "outputs": [],
   "source": [
    "# Start FastAPI + vLLM server in the background\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from jsonschema import ValidationError as _ValidationError\n",
    "from jsonschema import validate as _validate\n",
    "from pydantic import BaseModel\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # small model for Colab\n",
    "SYSTEM_RULES = (\n",
    "    \"Return ONLY valid compact JSON with keys: taxonomy (list[str]), \"\n",
    "    \"scores (object of label->score), confidence (0..1), notes (string). \"\n",
    "    \"If unsure, use [\\\"OTHER.Unknown\\\"] with low confidence.\"\n",
    ")\n",
    "\n",
    "_llm: Optional[LLM] = None\n",
    "app = FastAPI(title=\"Colab Llama Mapper Demo\")\n",
    "\n",
    "\n",
    "def _get_llm() -> LLM:\n",
    "    global _llm\n",
    "    if _llm is None:\n",
    "        _llm = LLM(model=MODEL_NAME, trust_remote_code=True)\n",
    "    return _llm\n",
    "\n",
    "\n",
    "def build_prompt(detector: str, output: str, metadata: Optional[Dict[str, Any]] = None) -> str:\n",
    "    md = f\"\\nMetadata: {json.dumps(metadata)}\" if metadata else \"\"\n",
    "    return (\n",
    "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{SYSTEM_RULES}<|eot_id|>\"\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\nDetector: {detector}\\nOutput: {output}{md}<|eot_id|>\"\n",
    "        f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "\n",
    "SCHEMA_PATH = os.environ.get(\"LLAMA_MAPPER_SCHEMA_PATH\")\n",
    "_SCHEMA = None\n",
    "if SCHEMA_PATH and os.path.exists(SCHEMA_PATH):\n",
    "    try:\n",
    "        with open(SCHEMA_PATH, \"r\", encoding=\"utf-8\") as fh:\n",
    "            _SCHEMA = json.load(fh)\n",
    "    except Exception as exc:\n",
    "        print(\"‚ö†Ô∏è Failed to load schema:\", exc)\n",
    "\n",
    "\n",
    "class MapRequest(BaseModel):\n",
    "    detector: str\n",
    "    output: str\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health() -> Dict[str, str]:\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "\n",
    "@app.post(\"/map\")\n",
    "def map_endpoint(req: MapRequest):\n",
    "    llm = _get_llm()\n",
    "    prompt = build_prompt(req.detector, req.output, req.metadata)\n",
    "    sampling = SamplingParams(temperature=0.1, top_p=0.9, max_tokens=200, stop=[\"</s>\", \"<|end|>\"])\n",
    "    result = llm.generate([prompt], sampling)[0].outputs[0].text.strip()\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(result)\n",
    "        if _SCHEMA is not None:\n",
    "            _validate(instance=parsed, schema=_SCHEMA)\n",
    "        return parsed\n",
    "    except (ValueError, _ValidationError):\n",
    "        return {\n",
    "            \"taxonomy\": [\"OTHER.Unknown\"],\n",
    "            \"scores\": {\"OTHER.Unknown\": 0.0},\n",
    "            \"confidence\": 0.0,\n",
    "            \"notes\": \"Fallback due to parsing/validation failure\",\n",
    "        }\n",
    "\n",
    "\n",
    "def _run_server():\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "\n",
    "thread = threading.Thread(target=_run_server, daemon=True)\n",
    "thread.start()\n",
    "time.sleep(5)\n",
    "print(\"‚úÖ API available at http://127.0.0.1:8000 (Colab local)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "serve_api_test"
   },
   "outputs": [],
   "source": [
    "# Quick test\n",
    "import requests, json\n",
    "payload = {\n",
    "  'detector': 'example',\n",
    "  'output': 'Detected email: user@example.com',\n",
    "  'metadata': {'src': 'colab-demo'}\n",
    "}\n",
    "r = requests.post('http://127.0.0.1:8000/map', json=payload, timeout=60)\n",
    "print(r.status_code)\n",
    "print(json.dumps(r.json(), indent=2)[:1200])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
