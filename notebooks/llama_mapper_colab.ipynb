{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ü¶ô Llama Mapper LoRA Fine-tuning on Google Colab\n",
    "\n",
    "This notebook fine-tunes Llama-3-8B-Instruct for mapping detector outputs to canonical taxonomy using LoRA.\n",
    "\n",
    "**‚ö†Ô∏è Important: Make sure to enable GPU runtime!**\n",
    "- Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU (T4)\n",
    "\n",
    "**Estimated time:** 30-60 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üîß Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Please enable GPU runtime.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers==4.36.0\n",
    "!pip install -q peft==0.7.0\n",
    "!pip install -q accelerate==0.25.0\n",
    "!pip install -q bitsandbytes==0.41.0\n",
    "!pip install -q datasets==2.15.0\n",
    "!pip install -q structlog\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone your repository (replace with your actual repo URL)\n",
    "!git clone https://github.com/your-username/llama-mapper.git\n",
    "%cd llama-mapper\n",
    "\n",
    "# Or upload your code files manually if not using git\n",
    "# You can upload the src/ directory using Colab's file browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "code_setup"
   },
   "source": [
    "## üìù Code Setup\n",
    "\n",
    "Since we can't clone the repo directly, let's create the necessary code inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_directories"
   },
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "import os\n",
    "os.makedirs('src/llama_mapper/training', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('model_checkpoints', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_loader_code"
   },
   "outputs": [],
   "source": [
    "%%writefile src/llama_mapper/training/model_loader.py\n",
    "\"\"\"\n",
    "ModelLoader for Llama-3-8B-Instruct with LoRA fine-tuning support.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModelLoader:\n",
    "    \"\"\"Loads and configures Llama-3-8B-Instruct model for LoRA fine-tuning.\"\"\"\n",
    "    \n",
    "    DEFAULT_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = DEFAULT_MODEL_NAME,\n",
    "        use_quantization: bool = True,\n",
    "        quantization_bits: int = 4,\n",
    "        use_fp16: bool = True,\n",
    "        device_map: str = \"auto\",\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.use_quantization = use_quantization\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.device_map = device_map\n",
    "    \n",
    "    def _get_quantization_config(self) -> Optional[BitsAndBytesConfig]:\n",
    "        if not self.use_quantization:\n",
    "            return None\n",
    "        \n",
    "        if self.quantization_bits == 4:\n",
    "            return BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "        elif self.quantization_bits == 8:\n",
    "            return BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_threshold=6.0,\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def load_tokenizer(self) -> PreTrainedTokenizer:\n",
    "        print(f\"Loading tokenizer: {self.model_name}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "        tokenizer.padding_side = \"left\"\n",
    "        return tokenizer\n",
    "    \n",
    "    def load_model(self) -> PreTrainedModel:\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        quantization_config = self._get_quantization_config()\n",
    "        torch_dtype = torch.float16 if self.use_fp16 else torch.float32\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=self.device_map,\n",
    "            torch_dtype=torch_dtype,\n",
    "            use_cache=False,\n",
    "        )\n",
    "        \n",
    "        if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "            model.gradient_checkpointing_enable()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def load_model_and_tokenizer(self) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "        tokenizer = self.load_tokenizer()\n",
    "        model = self.load_model()\n",
    "        \n",
    "        if len(tokenizer) != model.config.vocab_size:\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    def prepare_model_for_lora(self, model: PreTrainedModel, lora_config: LoraConfig) -> PeftModel:\n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "        \n",
    "        print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "        return peft_model\n",
    "    \n",
    "    @classmethod\n",
    "    def create_lora_config(\n",
    "        cls,\n",
    "        r: int = 8,\n",
    "        lora_alpha: int = 16,\n",
    "        target_modules: Optional[list] = None,\n",
    "        lora_dropout: float = 0.1,\n",
    "    ) -> LoraConfig:\n",
    "        if target_modules is None:\n",
    "            target_modules = [\"q_proj\", \"v_proj\"]\n",
    "        \n",
    "        return LoraConfig(\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=target_modules,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "def create_instruction_prompt(instruction: str, response: str = \"\") -> str:\n",
    "    \"\"\"Create instruction-following prompt format.\"\"\"\n",
    "    if response:\n",
    "        return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
    "    else:\n",
    "        return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_code"
   },
   "outputs": [],
   "source": [
    "%%writefile src/llama_mapper/training/colab_trainer.py\n",
    "\"\"\"\n",
    "Simplified LoRA trainer for Google Colab.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from .model_loader import ModelLoader, create_instruction_prompt\n",
    "\n",
    "@dataclass\n",
    "class ColabTrainingConfig:\n",
    "    \"\"\"Optimized config for Google Colab.\"\"\"\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    learning_rate: float = 2e-4\n",
    "    num_train_epochs: int = 1\n",
    "    max_sequence_length: int = 512\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    output_dir: str = \"./checkpoints\"\n",
    "\n",
    "class MapperDataset(Dataset):\n",
    "    \"\"\"Dataset for mapper training.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples: List[Dict[str, str]], tokenizer, max_length: int = 512):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        prompt = create_instruction_prompt(example[\"instruction\"], example[\"response\"])\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        labels = tokenized[\"input_ids\"].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels.squeeze(),\n",
    "        }\n",
    "\n",
    "class ColabTrainer:\n",
    "    \"\"\"Simplified trainer for Colab.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ColabTrainingConfig):\n",
    "        self.config = config\n",
    "        self.model_loader = ModelLoader(\n",
    "            use_quantization=True,\n",
    "            quantization_bits=4,\n",
    "        )\n",
    "    \n",
    "    def train(self, train_examples: List[Dict[str, str]]):\n",
    "        print(\"üîÑ Loading model and tokenizer...\")\n",
    "        base_model, tokenizer = self.model_loader.load_model_and_tokenizer()\n",
    "        \n",
    "        print(\"üîÑ Preparing LoRA model...\")\n",
    "        lora_config = self.model_loader.create_lora_config(\n",
    "            r=self.config.lora_r,\n",
    "            lora_alpha=self.config.lora_alpha,\n",
    "        )\n",
    "        model = self.model_loader.prepare_model_for_lora(base_model, lora_config)\n",
    "        \n",
    "        print(\"üîÑ Preparing dataset...\")\n",
    "        dataset = MapperDataset(train_examples, tokenizer, self.config.max_sequence_length)\n",
    "        \n",
    "        print(\"üîÑ Setting up training...\")\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            remove_unused_columns=False,\n",
    "        )\n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        \n",
    "        print(\"üöÄ Starting training...\")\n",
    "        result = trainer.train()\n",
    "        \n",
    "        print(\"üíæ Saving model...\")\n",
    "        trainer.save_model()\n",
    "        \n",
    "        return model, tokenizer, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## üìä Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_sample_data"
   },
   "outputs": [],
   "source": [
    "# Create sample training data\n",
    "training_data = [\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'User entered SSN: 123-45-6789'\",\n",
    "        \"response\": \"PII.Identifier.SSN\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Email found: user@example.com'\",\n",
    "        \"response\": \"PII.Contact.Email\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Phone number detected: (555) 123-4567'\",\n",
    "        \"response\": \"PII.Contact.Phone\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Credit card: 4111-1111-1111-1111'\",\n",
    "        \"response\": \"PII.Identifier.CreditCard\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Jailbreak attempt detected'\",\n",
    "        \"response\": \"JAILBREAK.Attempt\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Prompt injection: ignore previous instructions'\",\n",
    "        \"response\": \"PROMPT_INJECTION.PolicyOverride\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Address: 123 Main St, Anytown USA'\",\n",
    "        \"response\": \"PII.Contact.Address\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Bank account: 123456789'\",\n",
    "        \"response\": \"PII.Identifier.BankAccount\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Passport number: A12345678'\",\n",
    "        \"response\": \"PII.Identifier.Passport\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Tool request detected: execute command'\",\n",
    "        \"response\": \"PROMPT_INJECTION.ToolRequest\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Data exfiltration attempt'\",\n",
    "        \"response\": \"PROMPT_INJECTION.DataExfiltration\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Map this detector output to canonical taxonomy: 'Unknown prompt injection pattern'\",\n",
    "        \"response\": \"PROMPT_INJECTION.Other\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(training_data)} training examples\")\n",
    "print(\"\\nSample examples:\")\n",
    "for i, example in enumerate(training_data[:3]):\n",
    "    print(f\"{i+1}. {example['instruction']} ‚Üí {example['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## üèãÔ∏è Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_training"
   },
   "outputs": [],
   "source": [
    "# Add src to Python path\n",
    "import sys\n",
    "sys.path.append('/content/src')\n",
    "\n",
    "# Import our training components\n",
    "from llama_mapper.training.colab_trainer import ColabTrainer, ColabTrainingConfig\n",
    "\n",
    "# Create training configuration\n",
    "config = ColabTrainingConfig(\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    output_dir=\"./checkpoints\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration created\")\n",
    "print(f\"LoRA rank: {config.lora_r}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Batch size: {config.per_device_train_batch_size}\")\n",
    "print(f\"Epochs: {config.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ColabTrainer(config)\n",
    "\n",
    "# Run training\n",
    "print(\"üöÄ Starting LoRA fine-tuning...\")\n",
    "print(\"This will take about 15-30 minutes on T4 GPU\")\n",
    "\n",
    "model, tokenizer, results = trainer.train(training_data)\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Final loss: {results.training_loss:.4f}\")\n",
    "print(f\"Training time: {results.metrics['train_runtime']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_section"
   },
   "source": [
    "## üß™ Testing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_inference"
   },
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "from llama_mapper.training.model_loader import create_instruction_prompt\n",
    "\n",
    "def test_model(model, tokenizer, instruction):\n",
    "    \"\"\"Test the model with a given instruction.\"\"\"\n",
    "    prompt = create_instruction_prompt(instruction)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"Map this detector output to canonical taxonomy: 'SSN detected: 987-65-4321'\",\n",
    "    \"Map this detector output to canonical taxonomy: 'Email address: test@company.com'\",\n",
    "    \"Map this detector output to canonical taxonomy: 'Jailbreak pattern found'\",\n",
    "    \"Map this detector output to canonical taxonomy: 'Credit card number: 5555-4444-3333-2222'\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing the fine-tuned model:\\n\")\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    response = test_model(model, tokenizer, test_case)\n",
    "    print(f\"{i}. Input: {test_case}\")\n",
    "    print(f\"   Output: {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_section"
   },
   "source": [
    "## üíæ Save and Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "\n",
    "print(\"‚úÖ Model saved to ./final_model\")\n",
    "\n",
    "# Create a zip file for download\n",
    "!zip -r llama_mapper_lora.zip ./final_model\n",
    "\n",
    "print(\"‚úÖ Model packaged as llama_mapper_lora.zip\")\n",
    "print(\"You can download this file from the Colab file browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_info"
   },
   "outputs": [],
   "source": [
    "# Display model information\n",
    "import os\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "model_size = get_folder_size(\"./final_model\")\n",
    "zip_size = os.path.getsize(\"llama_mapper_lora.zip\") / (1024 * 1024)\n",
    "\n",
    "print(\"üìä Model Information:\")\n",
    "print(f\"Model folder size: {model_size:.1f} MB\")\n",
    "print(f\"Zip file size: {zip_size:.1f} MB\")\n",
    "print(f\"Training examples: {len(training_data)}\")\n",
    "print(f\"LoRA rank: {config.lora_r}\")\n",
    "print(f\"Target modules: q_proj, v_proj\")\n",
    "\n",
    "# List files in the model directory\n",
    "print(\"\\nüìÅ Model files:\")\n",
    "!ls -la ./final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Congratulations! You've successfully fine-tuned Llama-3-8B-Instruct for your mapper task. Here's what you can do next:\n",
    "\n",
    "### 1. Download Your Model\n",
    "- Download `llama_mapper_lora.zip` from the file browser\n",
    "- This contains your fine-tuned LoRA adapter\n",
    "\n",
    "### 2. Use the Model Locally\n",
    "```python\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"./final_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\n",
    "```\n",
    "\n",
    "### 3. Scale Up Training\n",
    "- Use larger datasets (100s-1000s of examples)\n",
    "- Train for more epochs\n",
    "- Experiment with different LoRA configurations\n",
    "\n",
    "### 4. Deploy the Model\n",
    "- Use the checkpoint manager for versioning\n",
    "- Deploy with your serving infrastructure\n",
    "- Set up A/B testing between model versions\n",
    "\n",
    "### 5. Evaluate Performance\n",
    "- Test on held-out validation data\n",
    "- Measure accuracy on your specific taxonomy\n",
    "- Compare against baseline models\n",
    "\n",
    "**Total training time:** ~15-30 minutes on T4 GPU  \n",
    "**Model size:** ~50-100 MB (LoRA adapter only)  \n",
    "**Cost:** Free on Google Colab!"
   ]
  },
   {
    "cell_type": "markdown",
    "metadata": {
     "id": "serve_api"
    },
    "source": [
     "## üöÄ Serve API on Colab\n",
     "\n",
     "Spin up a minimal FastAPI server backed by vLLM using a small model that fits on Colab GPU (e.g., Phi-3 Mini).\n",
     "This is for quick local testing; for production use the Docker/Helm setup in the repo."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
     "id": "serve_api_install"
    },
    "outputs": [],
    "source": [
     "# Install runtime deps for serving\n",
     "!pip -q install fastapi==0.110.0 uvicorn[standard]==0.29.0 vllm==0.4.2 nest-asyncio==1.6.0 jsonschema==4.20.0 requests==2.32.3\n",
     "import nest_asyncio, os, asyncio, threading, time, json\n",
     "nest_asyncio.apply()\n",
     "print('‚úÖ Serving dependencies installed')"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
     "id": "serve_api_code"
    },
    "outputs": [],
    "source": [
     "# Start FastAPI + vLLM server in the background\n",
     "from fastapi import FastAPI\n",
     "from pydantic import BaseModel\n",
     "from jsonschema import validate as _validate, ValidationError as _ValidationError\n",
     "from typing import Optional, Dict, Any\n",
     "\n",
     "MODEL_NAME = 'microsoft/Phi-3-mini-4k-instruct'  # small model for Colab\n",
     "\n",
     "# Minimal vLLM wrapper\n",
     "from vllm import LLM, SamplingParams\n",
     "_llm = None\n",
     "def _get_llm():\n",
     "    global _llm\n",
     "    if _llm is None:\n",
     "        _llm = LLM(model=MODEL_NAME, trust_remote_code=True)\n",
     "    return _llm\n",
     "\n",
     "# Simple prompt to produce strict JSON per our canonical format\n",
     "SYSTEM_RULES = (\n",
     "    'Return ONLY valid compact JSON with keys: taxonomy (list[str]), ' \
",
     "    'scores (object of label->score), confidence (0..1), notes (string). ' \
",
     "    'If unsure, use [\"OTHER.Unknown\"] with low confidence.'\n",
     ")\n",
     "\n",
     "def build_prompt(detector: str, output: str, metadata: Optional[Dict[str, Any]] = None) -> str:\n",
     "    md = f"\\nMetadata: {json.dumps(metadata)}" if metadata else ''\n",
     "    return (\n",
     "        f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{SYSTEM_RULES}<|eot_id|>"\n",
     "        f"<|start_header_id|>user<|end_header_id|>\\n\\nDetector: {detector}\\nOutput: {output}{md}<|eot_id|>"\n",
     "        f"<|start_header_id|>assistant<|end_header_id|>\\n\\n"\n",
     "    )\n",
     "\n",
     "# Optional JSON schema path (set these envs or upload files)\n",
     "SCHEMA_PATH = os.environ.get('LLAMA_MAPPER_SCHEMA_PATH')\n",
     "_SCHEMA = None\n",
     "if SCHEMA_PATH and os.path.exists(SCHEMA_PATH):\n",
     "    try:\n",
     "        import json as _json\n",
     "        with open(SCHEMA_PATH, 'r', encoding='utf-8') as f:\n",
     "            _SCHEMA = _json.load(f)\n",
     "    except Exception as e:\n",
     "        print('‚ö†Ô∏è Failed to load schema:', e)\n",
     "\n",
     "class MapRequest(BaseModel):\n",
     "    detector: str\n",
     "    output: str\n",
     "    metadata: Optional[Dict[str, Any]] = None\n",
     "\n",
     "app = FastAPI(title='Colab Llama Mapper Demo')\n",
     "\n",
     "@app.get('/health')\n",
     "def health():\n",
     "    return {'status': 'healthy'}\n",
     "\n",
     "@app.post('/map')\n",
     "def map_endpoint(req: MapRequest):\n",
     "    llm = _get_llm()\n",
     "    prompt = ''.join(build_prompt(req.detector, req.output, req.metadata))\n",
     "    sp = SamplingParams(temperature=0.1, top_p=0.9, max_tokens=200, stop=['</s>', '<|end|>'])\n",
     "    out = llm.generate([prompt], sp)[0].outputs[0].text.strip()\n",
     "    # Best-effort parse to JSON; fall back to OTHER.Unknown\n",
     "    try:\n",
     "        parsed = json.loads(out)\n",
     "        if _SCHEMA is not None:\n",
     "            _validate(instance=parsed, schema=_SCHEMA)\n",
     "        return parsed\n",
     "    except (ValueError, _ValidationError):\n",
     "        return {\n",
     "            'taxonomy': ['OTHER.Unknown'],\n",
     "            'scores': {'OTHER.Unknown': 0.0},\n",
     "            'confidence': 0.0,\n",
     "            'notes': 'Fallback due to parsing/validation failure'\n",
     "        }\n",
     "\n",
     "# Start Uvicorn in a background thread\n",
     "def _run():\n",
     "    import uvicorn\n",
     "    uvicorn.run(app, host='0.0.0.0', port=8000)\n",
     "thread = threading.Thread(target=_run, daemon=True)\n",
     "thread.start()\n",
     "time.sleep(5)\n",
     "print('‚úÖ API available at http://127.0.0.1:8000 (Colab local)')"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
     "id": "serve_api_test"
    },
    "outputs": [],
    "source": [
     "# Quick test\n",
     "import requests, json\n",
     "payload = {\n",
     "  'detector': 'example',\n",
     "  'output': 'Detected email: user@example.com',\n",
     "  'metadata': {'src': 'colab-demo'}\n",
     "}\n",
     "r = requests.post('http://127.0.0.1:8000/map', json=payload, timeout=60)\n",
     "print(r.status_code)\n",
     "print(json.dumps(r.json(), indent=2)[:1200])"
    ]
   }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}