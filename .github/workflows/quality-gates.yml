name: Quality Gates

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: "3.11"
  LLAMA_MAPPER_MODEL_VERSION: "mapper-lora@v0.0.0-ci"

jobs:
  quality-gates:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Lint with flake8
      run: |
        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
        
    - name: Format check with black
      run: |
        black --check --diff src/ tests/
        
    - name: Import sort check with isort
      run: |
        isort --check-only --diff src/ tests/
        
    - name: Type check with mypy
      run: |
        mypy src/llama_mapper/
        
    - name: Enforce version snapshot via CLI
      run: |
        python - <<'PY'
        import json, subprocess, sys
        out = subprocess.check_output([sys.executable, "-m", "llama_mapper.cli", "versions", "show"])
        snap = json.loads(out.decode())
        assert 'taxonomy' in snap and isinstance(snap['taxonomy'], dict), "taxonomy info missing"
        assert 'frameworks' in snap and isinstance(snap['frameworks'], dict), "frameworks info missing"
        # If version keys exist, they must be non-empty strings
        if 'version' in snap['taxonomy']:
            assert isinstance(snap['taxonomy']['version'], str) and snap['taxonomy']['version'], "taxonomy.version empty"
        if 'version' in snap['frameworks']:
            assert isinstance(snap['frameworks']['version'], str) and snap['frameworks']['version'], "frameworks.version empty"
        # Force non-unknown model version (CI sets LLAMA_MAPPER_MODEL_VERSION)
        assert snap.get('model_version', '') and snap['model_version'] != 'unknown', 'model_version must be set and not "unknown" in CI'
        print("Version snapshot OK")
        with open('version-snapshot.json', 'w', encoding='utf-8') as f:
            json.dump(snap, f, indent=2)
        PY

    - name: Upload version snapshot
      uses: actions/upload-artifact@v3
      with:
        name: version-snapshot
        path: version-snapshot.json

    - name: Comment PR with version snapshot
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          try {
            const snap = JSON.parse(fs.readFileSync('version-snapshot.json', 'utf8'));
            let body = `### ðŸ”– Version Snapshot (CI)\n`;
            body += `- Model: ${snap.model_version || 'unknown'}\n`;
            if (snap.taxonomy) body += `- Taxonomy: ${snap.taxonomy.version || 'n/a'} (${snap.taxonomy.file_path || ''})\n`;
            if (snap.frameworks) body += `- Frameworks: ${snap.frameworks.version || 'n/a'} (${snap.frameworks.file_path || ''})\n`;
            const detectors = snap.detectors ? Object.keys(snap.detectors).length : 0;
            body += `- Detectors loaded: ${detectors}\n`;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body
            });
          } catch (e) {
            core.warning('Failed to comment version snapshot: ' + e.message);
          }

    - name: Run unit tests
      run: |
        pytest tests/ -v --cov=src/llama_mapper --cov-report=xml --cov-report=term-missing
    
    - name: Run orchestrator conflict tests (coverage gate)
      run: |
        python -m pytest \
          tests/unit/test_conflict_resolution_strategies.py \
          tests/unit/test_conflict_resolution_hypothesis.py \
          tests/unit/test_conflict_resolution_opa.py \
          tests/integration/test_aggregator_conflict_metadata.py \
          tests/integration/test_opa_override_strategy.py \
          tests/integration/test_opa_failure_modes.py \
          tests/integration/test_metrics_and_logging_conflict.py \
          --cov=detector_orchestration.conflict --cov-report=term-missing --cov-fail-under=90 -q
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        
    - name: Generate comprehensive golden test cases
      run: |
        mkdir -p tests/
        python scripts/generate_golden_cases.py
        
    - name: Check golden test case coverage
      run: |
        python -m llama_mapper.cli quality check-coverage --golden-cases tests/golden_test_cases_comprehensive.json
        
    - name: Run quality gate validation
      run: |
        python -m llama_mapper.cli quality validate \
          --golden-cases tests/golden_test_cases_comprehensive.json \
          --output quality-report.json \
          --fail-on-error
          
    - name: Upload quality report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: quality-report
        path: quality-report.json
        
    - name: Comment PR with quality results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          try {
            const report = JSON.parse(fs.readFileSync('quality-report.json', 'utf8'));
            
            const status = report.overall_status === 'PASSED' ? 'âœ…' : 'âŒ';
            const summary = report.summary;
            
            let comment = `## ${status} Quality Gate Results\n\n`;
            comment += `**Overall Status:** ${report.overall_status}\n`;
            comment += `**Total Checks:** ${summary.total_checks}\n`;
            comment += `**Passed:** ${summary.passed_checks}\n`;
            comment += `**Failed:** ${summary.failed_checks}\n\n`;
            
            if (summary.failed_checks > 0) {
              comment += `### Failed Checks\n`;
              report.results.filter(r => r.status === 'FAILED').forEach(result => {
                comment += `- âŒ **${result.metric}**: ${result.message}\n`;
              });
              comment += `\n`;
            }
            
            if (report.recommendations && report.recommendations.length > 0) {
              comment += `### Recommendations\n`;
              report.recommendations.forEach(rec => {
                comment += `- ðŸ’¡ ${rec}\n`;
              });
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not read quality report:', error.message);
          }

  performance-tests:
    runs-on: ubuntu-latest
    needs: quality-gates
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install locust
        
    - name: Run performance smoke tests
      run: |
        # Create a simple performance test
        cat > performance_test.py << 'EOF'
        from locust import HttpUser, task, between
        import json
        
        class MapperUser(HttpUser):
            wait_time = between(1, 3)
            
            @task
            def test_map_endpoint(self):
                payload = {
                    "detector": "deberta-toxicity",
                    "output": "toxic",
                    "tenant_id": "test-tenant"
                }
                
                with self.client.post("/map", json=payload, catch_response=True) as response:
                    if response.status_code == 200:
                        data = response.json()
                        if "taxonomy" in data:
                            response.success()
                        else:
                            response.failure("Invalid response format")
                    else:
                        response.failure(f"HTTP {response.status_code}")
        EOF
        
        # Note: This would run against a test deployment in a real scenario
        echo "Performance test created (would run against test deployment)"
        
    - name: Validate latency requirements
      run: |
        echo "Latency validation would be performed against running service"
        echo "P95 latency should be â‰¤250ms for CPU deployment"
        echo "P95 latency should be â‰¤120ms for GPU deployment"

  security-scan:
    runs-on: ubuntu-latest
    needs: quality-gates
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Bandit security scan
      run: |
        pip install bandit[toml]
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ -f txt
        
    - name: Upload security report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-report
        path: bandit-report.json
        
    - name: Check for secrets
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD
        extra_args: --debug --only-verified