training:
  # Default training configuration
  default_config:
    # LoRA hyperparameters (optimized for Llama-3-8B-Instruct)
    lora_r: 16                    # LoRA rank as per guidelines
    lora_alpha: 32               # LoRA scaling parameter
    lora_dropout: 0.1
    target_modules:
      - "q_proj"
      - "k_proj" 
      - "v_proj"
      - "o_proj"
      # Additional modules for better performance
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    
    # Training hyperparameters (aligned with guidelines)
    learning_rate: 2e-4          # As specified in guidelines
    num_train_epochs: 3
    max_sequence_length: 2048
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 8
    warmup_steps: 100
    weight_decay: 0.01
    logging_steps: 10
    eval_steps: 100
    save_steps: 500
    
    # Memory optimization
    fp16: true
    gradient_checkpointing: true
    dataloader_num_workers: 4
    
    # Output configuration
    output_dir: "./checkpoints"
    run_name: "llama-mapper-lora"

  # Model loading configuration
  model_loader:
    default_model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
    use_quantization: false
    quantization_bits: 8
    use_fp16: true
    device_map: "auto"
    trust_remote_code: false

  # Checkpoint management
  checkpoint_manager:
    base_dir: "./model_checkpoints"
    version_prefix: "mapper-lora"
    max_versions: 10

  # Version management
  version_manager:
    models_dir: "./models"
    registry_file: "model_registry.json"

# Deployment configuration
deployment:
  canary:
    default_percentage: 5.0
    evaluation_duration: 3600  # 1 hour
    kpi_thresholds:
      p95_latency_improvement: 0.1
      schema_pass_rate_min: 0.95
      f1_score_improvement: 0.02
      error_rate_max: 0.01
    auto_promote: false

  ab_testing:
    default_traffic_split: 0.5
    min_sample_size: 1000
    confidence_level: 0.95